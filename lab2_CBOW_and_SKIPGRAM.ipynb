{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab2-CBOW and SKIPGRAM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsu5gXKBSDC_"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.linalg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-haDiAlS1QK",
        "outputId": "d54b43f3-8cc1-435f-c9df-59773119734e"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHdtZE1STDEY",
        "outputId": "04704d14-2f5f-4ab2-b5d2-d3d94224ef0d"
      },
      "source": [
        "# CBOW is a window view; we are trying to infer the word in the middle.\n",
        "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
        "\n",
        "raw_text= \"\"\"In text retrieval, full-text search refers to techniques for searching a single computer-stored document or a collection in a full-text database. \n",
        "Full-text search is distinguished from searches based on metadata or on parts of the original texts represented in databases such as titles, \n",
        "abstracts, selected sections, or bibliographical references.In a full-text search, a search engine examines all of the words in every stored document as it tries to match search criteria \n",
        "(for example, text specified by a user). Full-text-searching techniques became common in online bibliographic databases in the 1990s.Many websites and application programs \n",
        "such as word processing software provide full-text-search capabilities. Some web search engines, such as AltaVista, employ full-text-search techniques, \n",
        "while others index only a portion of the web pages examined by their indexing systems.When dealing with a small number of documents, it is possible for the full-text-search engine to directly scan the contents of the documents with each query, \n",
        "a strategy called serial scanning.This is what some tools, such as grep, do when searching.\"\"\".split()\n",
        "\n",
        "# By deriving a set from \"raw_text\", we deduplicate the array\n",
        "vocab = set(raw_text)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Basic Tokenizer\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "print(len(raw_text))\n",
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "166\n",
            "112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tOaMUu4TFuH",
        "outputId": "d3bca825-ec8c-4eaf-dc25-b1f6f2798be5"
      },
      "source": [
        "# Now lets create a \"dataset\"\n",
        "data = []\n",
        "for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n",
        "    context = []\n",
        "    for j in range(CONTEXT_SIZE, 0, -1):\n",
        "        context.append(raw_text[i - j])\n",
        "\n",
        "    for j in range(1, CONTEXT_SIZE + 1):\n",
        "        context.append(raw_text[i + j])\n",
        "        \n",
        "    target = raw_text[i]\n",
        "    data.append((context, target))\n",
        "print(data[:5])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['In', 'text', 'full-text', 'search'], 'retrieval,'), (['text', 'retrieval,', 'search', 'refers'], 'full-text'), (['retrieval,', 'full-text', 'refers', 'to'], 'search'), (['full-text', 'search', 'to', 'techniques'], 'refers'), (['search', 'refers', 'techniques', 'for'], 'to')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCzlEOmWTHTT"
      },
      "source": [
        "class CBOW(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, context, hidden_size):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(context*embed_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, vocab_size),\n",
        "            nn.LogSoftmax(dim = -1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "#         print(inputs.shape)\n",
        "#         print(inputs)\n",
        "        out = self.embedding(inputs)\n",
        "#         print(out.shape)\n",
        "        out = out.view(1, -1)\n",
        "#         print(out.shape)\n",
        "        out = self.linear(out)\n",
        "#         print(out.shape)\n",
        "        return out\n",
        "    \n",
        "    # This is what we are actually interested on\n",
        "    def get_word_vector(self, word):\n",
        "        out = self.embedding(word)\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4xiMoCpTI0x",
        "outputId": "83ef8bd3-db3e-4fce-d986-a4ae47ea437a"
      },
      "source": [
        "VOCAB_SIZE = len(vocab)\n",
        "EMBEDD_DIM = 10\n",
        "BATCH_SIZE = 6\n",
        "FULL_CONTEXT_SIZE = CONTEXT_SIZE * 2\n",
        "HIDDEN_SIZE = 256\n",
        "\n",
        "example_tensor = torch.randint(0, VOCAB_SIZE, [BATCH_SIZE, FULL_CONTEXT_SIZE]).to(device='cuda')\n",
        "print(example_tensor)\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 69,  39,  13, 106],\n",
            "        [ 53,  94,  18,  91],\n",
            "        [111,  32,  17,  28],\n",
            "        [ 56, 111,   9,  64],\n",
            "        [ 81, 111,  39,  23],\n",
            "        [ 81,  30,  51,  53]], device='cuda:0')\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arypin7ZTLpc",
        "outputId": "1454b0c9-e9e6-4a83-d85f-95aa91432d04"
      },
      "source": [
        "CBOW_embedding = nn.Embedding(VOCAB_SIZE, EMBEDD_DIM).to(device='cuda')\n",
        "\n",
        "example_result = CBOW_embedding(example_tensor)\n",
        "# Now we have a representation of the words in a vector of EMBEDD_DIM Dimensions\n",
        "print(example_result.shape)\n",
        "# example_result = torch.flatten(example_result, start_dim=1)\n",
        "example_result = example_result.view(BATCH_SIZE, -1)\n",
        "print(example_result.shape)\n",
        "\n",
        "\n",
        "CBOW_hidden = nn.Linear(EMBEDD_DIM * FULL_CONTEXT_SIZE, HIDDEN_SIZE).to(device='cuda')\n",
        "\n",
        "CBOW_hidden_relu = nn.ReLU()\n",
        "example_result = CBOW_hidden(example_result)\n",
        "example_result = CBOW_hidden_relu(example_result)\n",
        "print(example_result.shape)\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 4, 10])\n",
            "torch.Size([6, 40])\n",
            "torch.Size([6, 256])\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlCzvjGlTNmV",
        "outputId": "4c59adf9-0bb0-4d01-c902-6f85487d5eca"
      },
      "source": [
        "CBOW_output = nn.Linear(HIDDEN_SIZE, VOCAB_SIZE).to(device='cuda')\n",
        "\n",
        "CBOW_output_soft = nn.LogSoftmax(dim = -1)\n",
        "example_result = CBOW_output(example_result)\n",
        "example_result = CBOW_output_soft(example_result)\n",
        "print(example_result.shape)\n",
        "print(device)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 112])\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3I7Af2BzTRFl",
        "outputId": "632292a3-c310-4fa8-fe54-d9df97d5ec4a"
      },
      "source": [
        "print(example_result[0].argmax(-1))\n",
        "print(example_result[0])\n",
        "print(example_result[1].argmax(-1))\n",
        "print(example_result[1])\n",
        "# print(example_result[2].argmax(-1))\n",
        "# print(example_result[3].argmax(-1))\n",
        "# print(example_result[4].argmax(-1))\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(105, device='cuda:0')\n",
            "tensor([-5.0558, -5.2768, -4.7627, -4.7145, -4.5963, -4.6088, -4.5524, -4.3121,\n",
            "        -4.5132, -4.7526, -4.8646, -4.9905, -4.8123, -4.6209, -4.6675, -4.5826,\n",
            "        -4.9897, -4.7059, -4.7516, -4.8466, -4.6100, -4.7823, -4.3319, -4.2959,\n",
            "        -4.8649, -4.9223, -4.8116, -4.3349, -4.3782, -4.5373, -4.7756, -4.6252,\n",
            "        -4.5847, -5.1595, -4.8907, -4.5817, -5.0192, -4.6256, -5.1471, -4.6095,\n",
            "        -4.2658, -4.6264, -5.4936, -4.6324, -4.6546, -4.7912, -5.1694, -4.8234,\n",
            "        -4.5502, -5.0332, -4.9396, -4.9764, -4.8820, -5.0477, -4.2381, -4.4675,\n",
            "        -5.0875, -4.5827, -4.6923, -4.8908, -4.4941, -4.7939, -5.0441, -4.7967,\n",
            "        -4.8061, -4.5325, -5.2259, -4.8873, -4.8066, -4.7174, -4.7025, -4.5007,\n",
            "        -4.4148, -4.4291, -4.8298, -4.8457, -5.2504, -4.6443, -4.7679, -5.2180,\n",
            "        -5.4880, -4.6324, -4.7344, -4.5548, -4.6799, -4.5521, -4.7645, -5.1476,\n",
            "        -4.4434, -4.7676, -4.5460, -4.5790, -4.8169, -5.0828, -4.7807, -4.6402,\n",
            "        -4.7952, -4.5879, -4.8245, -4.6401, -5.0440, -4.4413, -4.9791, -4.6595,\n",
            "        -4.6633, -4.1086, -5.1534, -4.7575, -4.7750, -4.7263, -5.1598, -4.2569],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "tensor(40, device='cuda:0')\n",
            "tensor([-4.7118, -5.2762, -4.9417, -4.4404, -4.7412, -4.5199, -4.5613, -4.6054,\n",
            "        -4.6945, -4.5609, -4.8727, -4.9047, -4.9796, -4.7647, -4.7805, -4.7094,\n",
            "        -4.3588, -4.9065, -4.7980, -5.0020, -4.5542, -4.3060, -4.6337, -4.4805,\n",
            "        -4.8844, -4.6121, -4.9259, -5.1454, -4.7222, -4.3164, -4.5251, -4.8599,\n",
            "        -4.5108, -5.1867, -4.7744, -4.6603, -4.7518, -4.7515, -4.7188, -4.5745,\n",
            "        -3.8794, -4.5014, -5.1405, -5.0171, -4.8389, -4.5574, -4.9789, -4.7925,\n",
            "        -4.7458, -4.9555, -4.5737, -4.4376, -4.5765, -5.0571, -4.4081, -4.5940,\n",
            "        -4.9336, -5.1729, -4.6101, -4.7670, -4.6538, -4.3747, -4.5232, -4.5939,\n",
            "        -4.8697, -4.8912, -5.0920, -4.5194, -4.8558, -4.9082, -4.6164, -4.5803,\n",
            "        -4.9310, -4.4382, -5.0561, -4.6887, -4.7525, -4.4886, -4.8542, -5.5044,\n",
            "        -4.7807, -4.4899, -5.0483, -4.9397, -5.0546, -4.0983, -4.8871, -5.0020,\n",
            "        -4.7568, -5.1340, -4.1824, -4.6218, -4.7491, -4.9833, -4.7191, -4.7515,\n",
            "        -5.0361, -5.1239, -5.4778, -4.8822, -4.9874, -4.1701, -4.7767, -4.5122,\n",
            "        -5.0878, -4.5730, -5.0649, -5.0497, -4.9308, -4.7501, -4.7613, -4.4862],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWV0ighQTSi0",
        "outputId": "46646e39-3357-48ca-e149-573fdacafed8"
      },
      "source": [
        "# Simple helper method to transform the context to the expected int vector - tensor\n",
        "\n",
        "def make_context_vector(context, word_to_ix):\n",
        "    idxs = [word_to_ix[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "make_context_vector(data[0][0], word_to_ix)\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgK9iIGITTxv"
      },
      "source": [
        "def train(model, epochs, data, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for context, target in data:\n",
        "\n",
        "            # Prepare inputs and targets \n",
        "            context_idxs = make_context_vector(context, word_to_ix)\n",
        "            context_idxs = context_idxs.to(device)\n",
        "            target_id = make_context_vector([target], word_to_ix)\n",
        "            target_id = target_id.to(device)\n",
        "\n",
        "            # Do not accumulate \n",
        "            model.zero_grad()\n",
        "\n",
        "            # Step 3. Run the forward pass\n",
        "            log_probs = model(context_idxs)\n",
        "    #         break\n",
        "\n",
        "            # Step 4. Compute your loss function.\n",
        "            loss = loss_fn(log_probs, target_id)\n",
        "\n",
        "    #         loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
        "\n",
        "            # Step 5. Do the backward pass and update the gradient\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "            total_loss += loss.item()\n",
        "        losses.append(total_loss)\n",
        "    return losses\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05OQc1SWTVCK",
        "outputId": "02b4ddde-be0b-4ed0-ad3b-7ca730b8a334"
      },
      "source": [
        "VOCAB_SIZE = len(vocab)\n",
        "EMBEDD_DIM = 10\n",
        "BATCH_SIZE = 6\n",
        "FULL_CONTEXT_SIZE = CONTEXT_SIZE * 2\n",
        "HIDDEN_SIZE = 256\n",
        "\n",
        "loss_function = nn.NLLLoss() # Because we are using Log_softmax\n",
        "model = CBOW(vocab_size, EMBEDD_DIM, FULL_CONTEXT_SIZE, HIDDEN_SIZE)\n",
        "model = model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "lossesCBOW = train(model, 100, data, optimizer, loss_function)\n",
        "model.eval()\n",
        "\n",
        "print(lossesCBOW)  # The loss decreased every iteration over the training data!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[763.6366374492645, 754.586905002594, 745.7141070365906, 737.0117332935333, 728.4742207527161, 720.0976560115814, 711.8756289482117, 703.8017141819, 695.8725395202637, 688.0854001045227, 680.4357118606567, 672.9172267913818, 665.524908542633, 658.2507607936859, 651.0840373039246, 644.0180397033691, 637.041854262352, 630.1462798118591, 623.3186938762665, 616.5522645711899, 609.8371688127518, 603.1610062122345, 596.517971098423, 589.9010387659073, 583.304720044136, 576.7250691652298, 570.156611084938, 563.5910363197327, 557.0245035290718, 550.455904006958, 543.8841069936752, 537.3061303496361, 530.72320535779, 524.136483579874, 517.5395446717739, 510.9313420057297, 504.31455248594284, 497.6849080324173, 491.0447569489479, 484.3965540230274, 477.73818722367287, 471.0729140341282, 464.39693762362003, 457.7120953500271, 451.0183807015419, 444.31834268569946, 437.6125331521034, 430.9050825536251, 424.1885405629873, 417.4719637185335, 410.75493824481964, 404.0381329357624, 397.3229997307062, 390.6136911511421, 383.90631633996964, 377.2097188681364, 370.5234415009618, 363.84786562621593, 357.19075111299753, 350.55127554386854, 343.9345842450857, 337.34178707003593, 330.77325201034546, 324.2386498749256, 317.7343799471855, 311.2650075852871, 304.833997361362, 298.44643254578114, 292.1014234647155, 285.805909819901, 279.56215737760067, 273.3705712854862, 267.2388710528612, 261.16768792644143, 255.1634532287717, 249.22169817239046, 243.3518313653767, 237.55416725575924, 231.83442677557468, 226.19240683689713, 220.63340781629086, 215.1590644568205, 209.77096470072865, 204.47343700379133, 199.26447577401996, 194.1485287696123, 189.12904016673565, 184.20373849198222, 179.3786377236247, 174.64870816469193, 170.01747770607471, 165.48565678671002, 161.05451979488134, 156.72455643489957, 152.49550029449165, 148.36917218752205, 144.34351084195077, 140.42107861116529, 136.5985118597746, 132.87682423368096]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CL5mMBvVTWV6",
        "outputId": "16b2f19f-2a17-49b4-e606-6eed40c516ca"
      },
      "source": [
        "# list out keys and values separately\n",
        "key_list = list(word_to_ix.keys())\n",
        "val_list = list(word_to_ix.values())\n",
        "def similarity_cbow(word_1, word_2):\n",
        "    \n",
        "    # test word similarity\n",
        "    print(word_1)\n",
        "    print(word_2)\n",
        "    w1_id = torch.tensor(word_to_ix[word_1], dtype=torch.long)\n",
        "    w2_id = torch.tensor(word_to_ix[word_2], dtype=torch.long)\n",
        "    w1_id = w1_id.to(device)\n",
        "    w2_id = w2_id.to(device)\n",
        "    \n",
        "    word_1_vec = model.get_word_vector(w1_id)\n",
        "    word_2_vec = model.get_word_vector(w2_id)\n",
        "    \n",
        "    # The norm of a vector (1D-matrix) is the square root of the sum of all the squared values within the vector.\n",
        "    print(math.sqrt(torch.square(word_1_vec).sum()))    \n",
        "    print(torch.linalg.norm(word_1_vec))\n",
        "    print(torch.linalg.norm(word_2_vec))\n",
        "    print(word_1_vec.dot(word_2_vec))\n",
        "    \n",
        "    word_distance = torch.linalg.norm(word_1_vec - word_2_vec)\n",
        "    print(\"Distance between '{}' & '{}' : {:0.4f}\".format(word_1, word_2, word_distance))\n",
        "    word_similarity = (word_1_vec.dot(word_2_vec) / (torch.linalg.norm(word_1_vec) * torch.linalg.norm(word_2_vec)))\n",
        "    print(\"Similarity between '{}' & '{}' : {:0.4f}\".format(word_1, word_2, word_similarity))\n",
        "similarity_cbow(\"full-text\", \"search\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "full-text\n",
            "search\n",
            "4.541694127737662\n",
            "tensor(4.5417, device='cuda:0', grad_fn=<CopyBackwards>)\n",
            "tensor(3.2998, device='cuda:0', grad_fn=<CopyBackwards>)\n",
            "tensor(-2.3952, device='cuda:0', grad_fn=<DotBackward0>)\n",
            "Distance between 'full-text' & 'search' : 6.0254\n",
            "Similarity between 'full-text' & 'search' : -0.1598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAefejYVTZnQ"
      },
      "source": [
        "def predict_middle_word(prev_words, post_words):\n",
        "    prev_words = prev_words.split()\n",
        "    post_words = post_words.split()\n",
        "\n",
        "    input_words= make_context_vector(prev_words + post_words, word_to_ix)\n",
        "    input_words = input_words.to(device)\n",
        "    output = model(input_words)\n",
        "    out_ind = output.argmax(1)\n",
        "#     print(word_to_ix)\n",
        "#     out_word = word_to_ix.itos[out_ind.item()]\n",
        "    out_word = key_list[val_list.index(out_ind.item())]\n",
        "    print(out_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPmQu-gWTbvi",
        "outputId": "c26ea475-3e94-4d5f-fe9a-9f0203ed8f4e"
      },
      "source": [
        "predict_middle_word(\"full-text search\", \"to techniques\")\n",
        "predict_middle_word(\"Full-text search\", \"distinguished from \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "refers\n",
            "is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-gNPJNKTkil"
      },
      "source": [
        "Now that you saw how to create the CBOW model (word2vec), you should work on doing the \"opposite\" model, Skip-Gram\n",
        "\n",
        "Skip-gram as you saw on the lectures, reverses the problem so you need to predict through the \"fake task\" the context of the input\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhSb-xo4TlZ2"
      },
      "source": [
        "class Skipgram(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, contextSizeIn):\n",
        "        super(Skipgram, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linearFirst = nn.Linear(contextSizeIn*embedding_dim, 256)\n",
        "        self.linearSecond = nn.Linear(256, vocab_size)\n",
        "        self.contextSize = contextSizeIn\n",
        "        #self.Vembeddings = nn.Embedding(vocab_sze, embedding_dim)\n",
        "        #self.embedding_dim = embedding_dim\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs).view((1,-1))\n",
        "        linFirstOut = F.relu(self.linearFirst(embeddings))\n",
        "        linSecondOut = self.linearSecond(linFirstOut)\n",
        "        logProbabilities = F.log_softmax(linSecondOut, dim=1)#.view(contextSize, -1)\n",
        "        return logProbabilities\n",
        "    def get_word_vector(self, word):\n",
        "        out = self.embedding(word)\n",
        "        return out \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CYbzWBFToLt",
        "outputId": "9b56152b-912f-476f-cd5a-663d7281f600"
      },
      "source": [
        "\n",
        "losses = []\n",
        "lossFunction = nn.NLLLoss()\n",
        "modelSKIPGRAM = Skipgram(vocab_size, EMBEDD_DIM, FULL_CONTEXT_SIZE).to(device='cuda')\n",
        "optimizer = optim.SGD(modelSKIPGRAM.parameters(), lr=0.001)\n",
        "\n",
        "lossesSKIPGRAM = train(modelSKIPGRAM, 100, data, optimizer, lossFunction)\n",
        "\n",
        "modelSKIPGRAM.eval()\n",
        "print(lossesSKIPGRAM)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[768.51584815979, 760.3814251422882, 752.4013042449951, 744.5687782764435, 736.8795006275177, 729.331107378006, 721.9164290428162, 714.6358025074005, 707.4851417541504, 700.4603791236877, 693.5564687252045, 686.7672955989838, 680.0870931148529, 673.5092389583588, 667.0286347866058, 660.637082695961, 654.3286379575729, 648.0972534418106, 641.9334415197372, 635.8313617706299, 629.7829107046127, 623.7842313051224, 617.8294861316681, 611.9129755496979, 606.0280883312225, 600.1731289625168, 594.3452273607254, 588.5361127853394, 582.743234038353, 576.9627338647842, 571.1899589300156, 565.4227484464645, 559.6595476269722, 553.9010642170906, 548.1417244076729, 542.3838433623314, 536.6249312758446, 530.8630304336548, 525.0946083068848, 519.3204051852226, 513.5403293967247, 507.7536745071411, 501.9599792957306, 496.1563643813133, 490.3432182073593, 484.51973205804825, 478.6843473315239, 472.8398398756981, 466.9810518026352, 461.11416456103325, 455.23841857910156, 449.3543047308922, 443.4585416316986, 437.55532282590866, 431.6442515552044, 425.72224140167236, 419.79366943240166, 413.855204641819, 407.91476050019264, 401.9689785838127, 396.0179604291916, 390.06734323501587, 384.1163849234581, 378.1673767566681, 372.21953612565994, 366.27927708625793, 360.340505361557, 354.4141765385866, 348.49464121460915, 342.58837923407555, 336.69248428940773, 330.8162729740143, 324.9576304256916, 319.11849465966225, 313.3047180324793, 307.5145560801029, 301.7552449256182, 296.02787782251835, 290.3306626677513, 284.66873790323734, 279.0425697863102, 273.4563160613179, 267.91171891987324, 262.41157231479883, 256.96286035329103, 251.56080512702465, 246.2139734402299, 240.92282328754663, 235.68763014674187, 230.5144143626094, 225.40367867052555, 220.35557385534048, 215.37427350878716, 210.46360024809837, 205.62507785111666, 200.85404477268457, 196.1626666933298, 191.54419833421707, 187.0054124519229, 182.54590429365635]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jih6our1Tu9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecbd0f43-c1e4-4d37-fcb0-e8cee1e9a372"
      },
      "source": [
        "def predict_middle_word_SKIPGRAM(prev_words, post_words):\n",
        "    prev_words = prev_words.split()\n",
        "    post_words = post_words.split()\n",
        "\n",
        "    input_words= make_context_vector(prev_words + post_words, word_to_ix)\n",
        "    input_words = input_words.to(device)\n",
        "    output = modelSKIPGRAM(input_words)\n",
        "    out_ind = output.argmax(1)\n",
        "#     print(word_to_ix)\n",
        "#     out_word = word_to_ix.itos[out_ind.item()]\n",
        "    out_word = key_list[val_list.index(out_ind.item())]\n",
        "    print(out_word)\n",
        "predict_middle_word_SKIPGRAM(\"full-text search\", \"to techniques\")\n",
        "predict_middle_word_SKIPGRAM(\"Full-text search\", \"distinguished from\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "refers\n",
            "is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwNkPS5WTy1j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c40ff130-ec33-4ec4-a340-712bc3d86fc5"
      },
      "source": [
        "def similarity_SKIPGRAM(word_1, word_2):\n",
        "    \n",
        "    # test word similarity\n",
        "    print(word_1)\n",
        "    print(word_2)\n",
        "    w1_id = torch.tensor(word_to_ix[word_1], dtype=torch.long)\n",
        "    w2_id = torch.tensor(word_to_ix[word_2], dtype=torch.long)\n",
        "    w1_id = w1_id.to(device)\n",
        "    w2_id = w2_id.to(device)\n",
        "    \n",
        "    word_1_vec = modelSKIPGRAM.get_word_vector(w1_id)\n",
        "    word_2_vec = modelSKIPGRAM.get_word_vector(w2_id)\n",
        "    \n",
        "    # The norm of a vector (1D-matrix) is the square root of the sum of all the squared values within the vector.\n",
        "    print(math.sqrt(torch.square(word_1_vec).sum()))    \n",
        "    print(torch.linalg.norm(word_1_vec))\n",
        "    print(torch.linalg.norm(word_2_vec))\n",
        "    print(word_1_vec.dot(word_2_vec))\n",
        "    \n",
        "    word_distance = torch.linalg.norm(word_1_vec - word_2_vec)\n",
        "    print(\"Distance between '{}' & '{}' : {:0.4f}\".format(word_1, word_2, word_distance))\n",
        "    word_similarity = (word_1_vec.dot(word_2_vec) / (torch.linalg.norm(word_1_vec) * torch.linalg.norm(word_2_vec)))\n",
        "    print(\"Similarity between '{}' & '{}' : {:0.4f}\".format(word_1, word_2, word_similarity))\n",
        "similarity_SKIPGRAM(\"full-text\", \"search\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "full-text\n",
            "search\n",
            "2.440068774270296\n",
            "tensor(2.4401, device='cuda:0', grad_fn=<CopyBackwards>)\n",
            "tensor(2.5758, device='cuda:0', grad_fn=<CopyBackwards>)\n",
            "tensor(-0.9484, device='cuda:0', grad_fn=<DotBackward0>)\n",
            "Distance between 'full-text' & 'search' : 3.8060\n",
            "Similarity between 'full-text' & 'search' : -0.1509\n"
          ]
        }
      ]
    }
  ]
}