{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN vs LSTM vs GRU.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xrr-cylVoc63"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "# What will happen here?\n",
        "training_data = [\n",
        "    # Tags are: DET - determiner; NN - noun; V - verb\n",
        "    # For example, the word \"The\" is a determiner\n",
        "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"Jane walked into the room\".split(), [\"NN\", \"V\", \"DET\",\"DET\",\"NN\"]),\n",
        "    (\"Everybody does machine learning nowadays\".split(), [\"NN\", \"V\", \"NN\", \"NN\", \"ADV\"]),\n",
        "    (\"It was late in the day and everyone was walking home after a long day at work\".split(),\n",
        "     [\"NN\",\"V\",\"ADV\",\"DET\",\"DET\",\"NN\",\"DET\",\"NN\",\"V\",\"V\",\"NN\",\"ADV\",\"DET\",\"DET\",\"NN\",\"DET\",\"NN\"])\n",
        "    ]\n",
        "word_to_ix = {}\n",
        "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
        "for sent, tags in training_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
        "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
        "            \n",
        "print(word_to_ix)\n",
        "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2, \"ADV\": 3}  # Assign each tag with a unique index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "px_8cCJ3oqJp",
        "outputId": "89c03489-949b-43fc-eac2-571e21e8f52c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Jane': 5, 'walked': 6, 'into': 7, 'room': 8, 'Everybody': 9, 'does': 10, 'machine': 11, 'learning': 12, 'nowadays': 13, 'It': 14, 'was': 15, 'late': 16, 'in': 17, 'day': 18, 'and': 19, 'everyone': 20, 'walking': 21, 'home': 22, 'after': 23, 'a': 24, 'long': 25, 'at': 26, 'work': 27}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 1 #The hidden dimension is basically the number of nodes in each layer (like in the Multilayer Perceptron for example)\n",
        "HIDDEN_DIM = 11 #the number of nodes in each layer\n",
        "VOCAB_SIZE = len(word_to_ix)\n",
        "NUM_CLASSES = len(tag_to_ix)"
      ],
      "metadata": {
        "id": "9O8uhtjuoqNA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, criterion, epochs):\n",
        "    epoch_loss = []\n",
        "    for epoch in range(epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "        final_loss = 0\n",
        "        for sentence, tags in training_data:\n",
        "            \n",
        "            model.zero_grad()\n",
        "\n",
        "            # get inputs and targets ready for the network!\n",
        "            sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "            targets = prepare_sequence(tags, tag_to_ix)\n",
        "\n",
        "            # get the tag scores\n",
        "            tag_scores = model(sentence_in)\n",
        "            \n",
        "            loss = criterion(tag_scores, targets)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            final_loss += loss.item()\n",
        "        epoch_loss.append(final_loss)\n",
        "    \n",
        "    return epoch_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "CYtVRwhyoqQU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_sequence):\n",
        "    with torch.no_grad():\n",
        "        inputs = prepare_sequence(training_data[test_sequence][0], word_to_ix)\n",
        "        tag_scores = model(inputs)\n",
        "        \n",
        "        outputs = []\n",
        "        \n",
        "        print(tag_to_ix)\n",
        "        print(training_data[test_sequence][0])\n",
        "        print(training_data[test_sequence][1])\n",
        "        \n",
        "        for tag_score in tag_scores:\n",
        "            outputs.append(tag_score.topk(1).indices.item())\n",
        "            \n",
        "        print(outputs)\n",
        "        print(\"--------------\")"
      ],
      "metadata": {
        "id": "PT4z-l64o_NE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN tagger"
      ],
      "metadata": {
        "id": "t4nguHhq3dBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(RNNTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The RNN takes word embeddings as inputs, and outputs hidden states and output\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        \n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        rnn_out, _ = self.rnn(embeds.view(len(sentence),1, -1)) #The module is expecting [sentence_length, batch_size, embedding_dim]\n",
        "        \n",
        "        # in this case, rnn_out.view(len(sentence), -1) is the same as doing what function?\n",
        "        tag_space = self.hidden2tag(rnn_out.view(len(sentence), -1))\n",
        "        \n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "metadata": {
        "id": "YD7CXb6xpFSo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "losses = train(model, optimizer, loss_function, 200)\n",
        "print(losses)\n",
        "evaluate(model, 0)\n",
        "evaluate(model, 1)\n",
        "evaluate(model, 2)\n",
        "evaluate(model, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oeizIyIpK-8",
        "outputId": "5f8b9745-29cb-401b-ebc9-ee3c6d6f9143"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5.247547745704651, 5.051419138908386, 4.938004374504089, 4.854490399360657, 4.783879637718201, 4.71931004524231, 4.657251954078674, 4.59552788734436, 4.532626986503601, 4.467418789863586, 4.399017095565796, 4.3267346024513245, 4.25009685754776, 4.168894469738007, 4.08325469493866, 3.993701219558716, 3.901167333126068, 3.8069222569465637, 3.7124056220054626, 3.618972599506378, 3.527613341808319, 3.4387372732162476, 3.3521270155906677, 3.2670783400535583, 3.182667315006256, 3.0980424284934998, 3.0126612782478333, 2.9263972342014313, 2.839506894350052, 2.752471089363098, 2.6657872200012207, 2.5797893404960632, 2.494567096233368, 2.410003423690796, 2.3258964717388153, 2.24210087954998, 2.1586291939020157, 2.075689345598221, 1.9936655759811401, 1.913066104054451, 1.8344506621360779, 1.7583566009998322, 1.6852358430624008, 1.615419052541256, 1.5491089075803757, 1.4863946586847305, 1.4272720664739609, 1.371665507555008, 1.319445863366127, 1.2704474478960037, 1.2244815975427628, 1.1813505813479424, 1.1408573687076569, 1.1028134040534496, 1.0670414827764034, 1.033377479761839, 1.0016688592731953, 0.9717724807560444, 0.9435540735721588, 0.9168856628239155, 0.8916455581784248, 0.8677181862294674, 0.8449941016733646, 0.8233704902231693, 0.8027516603469849, 0.783048652112484, 0.7641800008714199, 0.7460709027945995, 0.728654008358717, 0.7118680588901043, 0.6956581398844719, 0.6799758020788431, 0.6647775880992413, 0.6500253770500422, 0.6356861107051373, 0.6217309255152941, 0.6081354450434446, 0.594878451898694, 0.5819422341883183, 0.5693120248615742, 0.5569754093885422, 0.5449216775596142, 0.5331419911235571, 0.5216283649206161, 0.5103741306811571, 0.4993729889392853, 0.48861901089549065, 0.47810703702270985, 0.46783157624304295, 0.4577878713607788, 0.4479708671569824, 0.4383759871125221, 0.42899891920387745, 0.4198351614177227, 0.41088069789111614, 0.4021314363926649, 0.3935839310288429, 0.38523435965180397, 0.37707933224737644, 0.3691159188747406, 0.361340781673789, 0.3537510111927986, 0.34634383395314217, 0.33911649510264397, 0.33206651732325554, 0.32519118674099445, 0.31848802976310253, 0.3119546100497246, 0.3055884800851345, 0.29938717745244503, 0.2933477880433202, 0.2874678038060665, 0.28174422588199377, 0.27617400232702494, 0.27075399458408356, 0.2654809569939971, 0.26035132072865963, 0.2553614191710949, 0.25050765834748745, 0.245786109007895, 0.2411933233961463, 0.23672493733465672, 0.23237749468535185, 0.22814717516303062, 0.2240302376449108, 0.22002290282398462, 0.21612195670604706, 0.21232344675809145, 0.20862417295575142, 0.20502104796469212, 0.2015105914324522, 0.19809007365256548, 0.19475633651018143, 0.19150650035589933, 0.18833779729902744, 0.18524782359600067, 0.18223385605961084, 0.17929336056113243, 0.17642419319599867, 0.1736239716410637, 0.1708903517574072, 0.1682215165346861, 0.16561543941497803, 0.16306984797120094, 0.16058307234197855, 0.15815331507474184, 0.15577893517911434, 0.1534579312428832, 0.15118895657360554, 0.1489705964922905, 0.1468010600656271, 0.1446790248155594, 0.14260317105799913, 0.14057210832834244, 0.13858456816524267, 0.13663934636861086, 0.13473522011190653, 0.132871201261878, 0.1310458928346634, 0.1292584603652358, 0.12750780023634434, 0.12579297460615635, 0.12411306705325842, 0.12246725987643003, 0.12085432186722755, 0.11927383486181498, 0.11772478651255369, 0.1162064503878355, 0.11471817549318075, 0.11325896997004747, 0.11182827968150377, 0.11042546201497316, 0.10904974862933159, 0.10770076513290405, 0.10637770500034094, 0.10508000664412975, 0.10380705539137125, 0.1025584526360035, 0.10133339557796717, 0.10013169143348932, 0.09895250201225281, 0.09779553860425949, 0.096660315990448, 0.09554617665708065, 0.09445286728441715, 0.0933798523619771, 0.09232661686837673, 0.09129280829802155, 0.09027814026921988, 0.08928183326497674, 0.08830373594537377, 0.08734361734241247, 0.08640067419037223, 0.08547474630177021, 0.084565460216254, 0.08367237960919738, 0.08279523067176342, 0.08193373493850231, 0.08108743280172348, 0.08025583811104298]\n",
            "{'DET': 0, 'NN': 1, 'V': 2, 'ADV': 3}\n",
            "['The', 'dog', 'ate', 'the', 'apple']\n",
            "['DET', 'NN', 'V', 'DET', 'NN']\n",
            "[0, 1, 2, 0, 1]\n",
            "--------------\n",
            "{'DET': 0, 'NN': 1, 'V': 2, 'ADV': 3}\n",
            "['Jane', 'walked', 'into', 'the', 'room']\n",
            "['NN', 'V', 'DET', 'DET', 'NN']\n",
            "[1, 2, 0, 0, 1]\n",
            "--------------\n",
            "{'DET': 0, 'NN': 1, 'V': 2, 'ADV': 3}\n",
            "['Everybody', 'does', 'machine', 'learning', 'nowadays']\n",
            "['NN', 'V', 'NN', 'NN', 'ADV']\n",
            "[1, 2, 1, 1, 3]\n",
            "--------------\n",
            "{'DET': 0, 'NN': 1, 'V': 2, 'ADV': 3}\n",
            "['It', 'was', 'late', 'in', 'the', 'day', 'and', 'everyone', 'was', 'walking', 'home', 'after', 'a', 'long', 'day', 'at', 'work']\n",
            "['NN', 'V', 'ADV', 'DET', 'DET', 'NN', 'DET', 'NN', 'V', 'V', 'NN', 'ADV', 'DET', 'DET', 'NN', 'DET', 'NN']\n",
            "[1, 2, 3, 0, 0, 1, 0, 1, 2, 2, 1, 3, 0, 0, 1, 0, 1]\n",
            "--------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = torch.FloatTensor(losses)\n",
        "perplexity  = torch.exp(b)\n",
        "print(\"PP:\", perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTONyQrZpOSG",
        "outputId": "a399ae9c-dca2-4dc7-d082-141b04d8ba5a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP: tensor([190.0995, 156.2441, 139.4916, 128.3153, 119.5673, 112.0909, 105.3462,\n",
            "         99.0404,  93.0026,  87.1315,  81.3709,  75.6967,  70.1122,  64.6439,\n",
            "         59.3383,  54.2553,  49.4602,  45.0117,  40.9522,  37.2992,  34.0426,\n",
            "         31.1476,  28.5634,  26.2346,  24.1110,  22.1545,  20.3415,  18.6603,\n",
            "         17.1073,  15.6813,  14.3793,  13.1944,  12.1165,  11.1340,  10.2359,\n",
            "          9.4131,   8.6593,   7.9700,   7.3424,   6.7738,   6.2617,   5.8029,\n",
            "          5.3937,   5.0300,   4.7073,   4.4211,   4.1673,   3.9419,   3.7413,\n",
            "          3.5624,   3.4024,   3.2588,   3.1295,   3.0126,   2.9068,   2.8105,\n",
            "          2.7228,   2.6426,   2.5691,   2.5015,   2.4391,   2.3815,   2.3280,\n",
            "          2.2782,   2.2317,   2.1881,   2.1472,   2.1087,   2.0723,   2.0378,\n",
            "          2.0050,   1.9738,   1.9441,   1.9156,   1.8883,   1.8621,   1.8370,\n",
            "          1.8128,   1.7895,   1.7671,   1.7454,   1.7245,   1.7043,   1.6848,\n",
            "          1.6659,   1.6477,   1.6301,   1.6130,   1.5965,   1.5806,   1.5651,\n",
            "          1.5502,   1.5357,   1.5217,   1.5081,   1.4950,   1.4823,   1.4700,\n",
            "          1.4580,   1.4465,   1.4353,   1.4244,   1.4139,   1.4037,   1.3938,\n",
            "          1.3843,   1.3750,   1.3661,   1.3574,   1.3490,   1.3409,   1.3330,\n",
            "          1.3254,   1.3181,   1.3110,   1.3041,   1.2974,   1.2909,   1.2847,\n",
            "          1.2786,   1.2728,   1.2671,   1.2616,   1.2563,   1.2511,   1.2461,\n",
            "          1.2413,   1.2365,   1.2320,   1.2276,   1.2232,   1.2191,   1.2150,\n",
            "          1.2111,   1.2072,   1.2035,   1.1999,   1.1964,   1.1929,   1.1896,\n",
            "          1.1864,   1.1832,   1.1801,   1.1771,   1.1742,   1.1713,   1.1686,\n",
            "          1.1659,   1.1632,   1.1606,   1.1581,   1.1557,   1.1533,   1.1509,\n",
            "          1.1486,   1.1464,   1.1442,   1.1421,   1.1400,   1.1380,   1.1360,\n",
            "          1.1340,   1.1321,   1.1303,   1.1285,   1.1267,   1.1249,   1.1232,\n",
            "          1.1216,   1.1199,   1.1183,   1.1168,   1.1152,   1.1137,   1.1122,\n",
            "          1.1108,   1.1094,   1.1080,   1.1066,   1.1053,   1.1040,   1.1027,\n",
            "          1.1015,   1.1003,   1.0991,   1.0979,   1.0967,   1.0956,   1.0945,\n",
            "          1.0934,   1.0923,   1.0913,   1.0902,   1.0892,   1.0882,   1.0873,\n",
            "          1.0863,   1.0854,   1.0845,   1.0836])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM tagger"
      ],
      "metadata": {
        "id": "408tWdPz3Xad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "metadata": {
        "id": "_6oCDVo9pU5_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function =  nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "losses_lstm = train(model, optimizer, loss_function, 250)\n",
        "print(losses_lstm)\n",
        "evaluate(model, 0)\n",
        "evaluate(model, 1)\n",
        "evaluate(model, 2)\n",
        "evaluate(model, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-vI2nVopbts",
        "outputId": "542ef792-4a23-4bb4-efe5-75d90e626dbc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5.393147110939026, 5.308363199234009, 5.241724252700806, 5.189165472984314, 5.147485375404358, 5.114192605018616, 5.087366580963135, 5.065536022186279, 5.047575235366821, 5.032623887062073, 5.0200241804122925, 5.009269118309021, 4.999967694282532, 4.991815090179443, 4.984573721885681, 4.978056192398071, 4.972112774848938, 4.966624140739441, 4.961493730545044, 4.956643342971802, 4.9520087242126465, 4.947536826133728, 4.943183898925781, 4.938913345336914, 4.934693336486816, 4.930498123168945, 4.9263060092926025, 4.922096848487854, 4.917854905128479, 4.913565278053284, 4.909215807914734, 4.904795527458191, 4.900294303894043, 4.895703911781311, 4.891016244888306, 4.886224150657654, 4.8813217878341675, 4.876302361488342, 4.871161341667175, 4.865893363952637, 4.860492944717407, 4.854956388473511, 4.849279284477234, 4.843457579612732, 4.837486624717712, 4.831363201141357, 4.825083494186401, 4.8186434507369995, 4.812040209770203, 4.805269360542297, 4.798327326774597, 4.791211128234863, 4.783917188644409, 4.776442050933838, 4.768782258033752, 4.760934472084045, 4.752895951271057, 4.7446630001068115, 4.736232161521912, 4.727601528167725, 4.718767166137695, 4.709726572036743, 4.700477719306946, 4.691017389297485, 4.681344270706177, 4.671455383300781, 4.661349058151245, 4.651024341583252, 4.6404789686203, 4.6297125816345215, 4.6187238693237305, 4.607512474060059, 4.596077919006348, 4.584420680999756, 4.572540521621704, 4.560438752174377, 4.548115253448486, 4.535572409629822, 4.5228111743927, 4.509833574295044, 4.496641516685486, 4.483237624168396, 4.4696245193481445, 4.4558045864105225, 4.441781759262085, 4.4275583028793335, 4.413138270378113, 4.398524880409241, 4.3837220668792725, 4.368733048439026, 4.353561520576477, 4.338211357593536, 4.322685897350311, 4.3069881200790405, 4.2911219000816345, 4.275089621543884, 4.258894205093384, 4.242538094520569, 4.226023256778717, 4.209351181983948, 4.192523419857025, 4.175540506839752, 4.158402383327484, 4.141109108924866, 4.123660206794739, 4.106054186820984, 4.088289022445679, 4.0703625082969666, 4.052271366119385, 4.034012615680695, 4.015582203865051, 3.99697482585907, 3.978186309337616, 3.9592105746269226, 3.940042197704315, 3.9206746220588684, 3.901100277900696, 3.8813127875328064, 3.8613044023513794, 3.8410671949386597, 3.8205928206443787, 3.799873411655426, 3.7789005041122437, 3.757664918899536, 3.7361586689949036, 3.7143726348876953, 3.692298650741577, 3.669928252696991, 3.647252678871155, 3.62426495552063, 3.6009570360183716, 3.577322006225586, 3.5533542037010193, 3.5290488600730896, 3.5044010281562805, 3.4794089794158936, 3.454070746898651, 3.4283878803253174, 3.40236234664917, 3.3760002851486206, 3.3493089079856873, 3.32229882478714, 3.2949836254119873, 3.2673799991607666, 3.23950731754303, 3.2113879323005676, 3.183046817779541, 3.154511034488678, 3.1258091926574707, 3.0969711542129517, 3.0680267810821533, 3.0390056371688843, 3.009935975074768, 2.980844259262085, 2.9517539739608765, 2.9226858019828796, 2.893656611442566, 2.8646798729896545, 2.835764229297638, 2.8069141507148743, 2.778131067752838, 2.749411404132843, 2.7207477688789368, 2.6921300292015076, 2.663543939590454, 2.634973108768463, 2.606398344039917, 2.577798843383789, 2.5491515398025513, 2.5204331278800964, 2.49161833524704, 2.4626824259757996, 2.4336010217666626, 2.404349148273468, 2.374904751777649, 2.345246136188507, 2.3153547942638397, 2.285214841365814, 2.254814863204956, 2.224147766828537, 2.1932129859924316, 2.162015974521637, 2.1305696070194244, 2.0988959670066833, 2.0670244991779327, 2.034993678331375, 2.0028492510318756, 1.9706441462039948, 1.9384345412254333, 1.9062799215316772, 1.8742388784885406, 1.842368245124817, 1.8107198774814606, 1.7793405950069427, 1.748270869255066, 1.7175446152687073, 1.6871903240680695, 1.6572305262088776, 1.6276837587356567, 1.5985640287399292, 1.569883018732071, 1.5416497886180878, 1.5138709843158722, 1.4865523278713226, 1.4596981406211853, 1.4333119690418243, 1.4073965847492218, 1.3819541037082672, 1.3569858968257904, 1.332493081688881, 1.3084758073091507, 1.2849342226982117, 1.2618672251701355, 1.2392732053995132, 1.2171505838632584, 1.1954961568117142, 1.1743063628673553, 1.1535775065422058, 1.1333045065402985, 1.1134820729494095, 1.094104290008545, 1.0751644223928452, 1.056655839085579, 1.0385706424713135, 1.0209013521671295, 1.0036396086215973, 0.9867771863937378, 0.9703051298856735, 0.9542150497436523, 0.9384976774454117, 0.92314413189888, 0.9081455171108246, 0.8934926837682724, 0.8791766166687012, 0.8651887774467468, 0.8515197932720184, 0.8381614536046982, 0.8251050859689713, 0.8123421967029572, 0.7998648881912231, 0.7876648753881454, 0.7757343351840973, 0.7640657424926758, 0.7526514753699303, 0.7414846122264862, 0.7305577099323273, 0.7198642864823341, 0.7093973159790039, 0.6991506442427635, 0.6891178637742996]\n",
            "{'DET': 0, 'NN': 1, 'V': 2, 'ADV': 3}\n",
            "['The', 'dog', 'ate', 'the', 'apple']\n",
            "['DET', 'NN', 'V', 'DET', 'NN']\n",
            "[0, 1, 2, 0, 1]\n",
            "--------------\n",
            "{'DET': 0, 'NN': 1, 'V': 2, 'ADV': 3}\n",
            "['Jane', 'walked', 'into', 'the', 'room']\n",
            "['NN', 'V', 'DET', 'DET', 'NN']\n",
            "[1, 2, 0, 0, 1]\n",
            "--------------\n",
            "{'DET': 0, 'NN': 1, 'V': 2, 'ADV': 3}\n",
            "['Everybody', 'does', 'machine', 'learning', 'nowadays']\n",
            "['NN', 'V', 'NN', 'NN', 'ADV']\n",
            "[1, 2, 1, 1, 3]\n",
            "--------------\n",
            "{'DET': 0, 'NN': 1, 'V': 2, 'ADV': 3}\n",
            "['It', 'was', 'late', 'in', 'the', 'day', 'and', 'everyone', 'was', 'walking', 'home', 'after', 'a', 'long', 'day', 'at', 'work']\n",
            "['NN', 'V', 'ADV', 'DET', 'DET', 'NN', 'DET', 'NN', 'V', 'V', 'NN', 'ADV', 'DET', 'DET', 'NN', 'DET', 'NN']\n",
            "[1, 2, 1, 0, 0, 1, 0, 1, 2, 2, 1, 3, 0, 0, 1, 0, 1]\n",
            "--------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = torch.FloatTensor(losses_lstm)\n",
        "perplexity  = torch.exp(b)\n",
        "print(\"PP:\", perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFRTm83CpbxJ",
        "outputId": "ac897276-224f-4f64-82b6-cf804ae91e08"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP: tensor([219.8943, 202.0192, 188.9957, 179.3189, 171.9984, 166.3664, 161.9628,\n",
            "        158.4654, 155.6446, 153.3348, 151.4150, 149.7952, 148.4083, 147.2034,\n",
            "        146.1413, 145.1918, 144.3315, 143.5415, 142.8069, 142.1159, 141.4588,\n",
            "        140.8277, 140.2160, 139.6185, 139.0305, 138.4485, 137.8692, 137.2902,\n",
            "        136.7090, 136.1239, 135.5331, 134.9353, 134.3293, 133.7141, 133.0887,\n",
            "        132.4525, 131.8048, 131.1448, 130.4724, 129.7868, 129.0878, 128.3751,\n",
            "        127.6484, 126.9074, 126.1519, 125.3818, 124.5969, 123.7971, 122.9823,\n",
            "        122.1524, 121.3074, 120.4472, 119.5718, 118.6813, 117.7757, 116.8551,\n",
            "        115.9195, 114.9691, 114.0039, 113.0242, 112.0301, 111.0218, 109.9997,\n",
            "        108.9640, 107.9150, 106.8531, 105.7787, 104.6922, 103.5940, 102.4846,\n",
            "        101.3646, 100.2345,  99.0949,  97.9464,  96.7897,  95.6254,  94.4542,\n",
            "         93.2769,  92.0941,  90.9067,  89.7153,  88.5208,  87.3239,  86.1254,\n",
            "         84.9261,  83.7267,  82.5281,  81.3308,  80.1358,  78.9435,  77.7549,\n",
            "         76.5705,  75.3908,  74.2166,  73.0484,  71.8866,  70.7317,  69.5842,\n",
            "         68.4445,  67.3128,  66.1896,  65.0750,  63.9692,  62.8725,  61.7850,\n",
            "         60.7067,  59.6378,  58.5782,  57.5280,  56.4871,  55.4556,  54.4332,\n",
            "         53.4201,  52.4159,  51.4208,  50.4345,  49.4568,  48.4878,  47.5273,\n",
            "         46.5752,  45.6313,  44.6955,  43.7679,  42.8483,  41.9366,  41.0328,\n",
            "         40.1370,  39.2491,  38.3691,  37.4972,  36.6333,  35.7776,  34.9303,\n",
            "         34.0915,  33.2615,  32.4405,  31.6289,  30.8269,  30.0350,  29.2535,\n",
            "         28.4830,  27.7240,  26.9770,  26.2425,  25.5211,  24.8135,  24.1201,\n",
            "         23.4416,  22.7783,  22.1308,  21.4994,  20.8845,  20.2861,  19.7044,\n",
            "         19.1395,  18.5912,  18.0592,  17.5434,  17.0434,  16.5587,  16.0889,\n",
            "         15.6334,  15.1917,  14.7631,  14.3470,  13.9429,  13.5502,  13.1681,\n",
            "         12.7962,  12.4340,  12.0808,  11.7363,  11.3999,  11.0712,  10.7500,\n",
            "         10.4358,  10.1285,   9.8278,   9.5335,   9.2456,   8.9640,   8.6886,\n",
            "          8.4197,   8.1572,   7.9013,   7.6522,   7.4101,   7.1753,   6.9479,\n",
            "          6.7280,   6.5159,   6.3115,   6.1148,   5.9259,   5.7447,   5.5708,\n",
            "          5.4043,   5.2448,   5.0921,   4.9459,   4.8061,   4.6723,   4.5443,\n",
            "          4.4218,   4.3047,   4.1926,   4.0853,   3.9827,   3.8845,   3.7905,\n",
            "          3.7005,   3.6144,   3.5320,   3.4531,   3.3775,   3.3052,   3.2359,\n",
            "          3.1695,   3.1059,   3.0449,   2.9865,   2.9305,   2.8767,   2.8252,\n",
            "          2.7757,   2.7282,   2.6826,   2.6387,   2.5966,   2.5561,   2.5172,\n",
            "          2.4797,   2.4436,   2.4089,   2.3755,   2.3432,   2.3121,   2.2821,\n",
            "          2.2532,   2.2252,   2.1983,   2.1722,   2.1470,   2.1226,   2.0990,\n",
            "          2.0762,   2.0542,   2.0328,   2.0120,   1.9920])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU tagger"
      ],
      "metadata": {
        "id": "F-vMHzYe3jML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(GRUTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The GRU takes word embeddings as inputs, and outputs hidden states and output\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        \n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        gru_out, _ = self.gru(embeds.view(len(sentence), 1, -1)) #The module is expecting [sentence_length, batch_size, embedding_dim]\n",
        "        \n",
        "        # in this case, gru_out.view(len(sentence), -1) is the same as doing what function?\n",
        "        tag_space = self.hidden2tag(gru_out.view(len(sentence), -1))\n",
        "        \n",
        "        tag_scores = F.softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "metadata": {
        "id": "dVDpHpqjpU9a"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_gru = GRUTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function =  nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_gru.parameters(), lr=0.1)\n",
        "losses_gru = train(model_gru, optimizer, loss_function, 200)\n",
        "print(losses_gru)\n",
        "evaluate(model_gru, 0)\n",
        "evaluate(model_gru, 1)\n",
        "evaluate(model_gru, 2)\n",
        "evaluate(model_gru, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycBalqv-qywr",
        "outputId": "e3434890-8e2f-4bde-e47b-97eaa2146df0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5.447351574897766, 5.4371432065963745, 5.427152156829834, 5.417380928993225, 5.4078357219696045, 5.398521780967712, 5.389446496963501, 5.380617141723633, 5.372041344642639, 5.363725185394287, 5.35567581653595, 5.347898602485657, 5.340397834777832, 5.333177208900452, 5.326237797737122, 5.319580674171448, 5.313204288482666, 5.307107090950012, 5.301283955574036, 5.2957305908203125, 5.290440559387207, 5.285406947135925, 5.28062117099762, 5.276074647903442, 5.271758317947388, 5.267661929130554, 5.263775587081909, 5.260089635848999, 5.256593704223633, 5.253277540206909, 5.250131130218506, 5.247145652770996, 5.244310021400452, 5.24161696434021, 5.239056468009949, 5.236620783805847, 5.234301805496216, 5.232092261314392, 5.229984283447266, 5.2279709577560425, 5.226047158241272, 5.22420608997345, 5.2224414348602295, 5.2207489013671875, 5.2191232442855835, 5.217559218406677, 5.216053128242493, 5.214600563049316, 5.213197588920593, 5.211840510368347, 5.210526704788208, 5.209251761436462, 5.208013892173767, 5.206810116767883, 5.2056368589401245, 5.204492807388306, 5.203375458717346, 5.202282190322876, 5.201210975646973, 5.200160503387451, 5.19912850856781, 5.198113560676575, 5.197113633155823, 5.196127533912659, 5.195153832435608, 5.194190740585327, 5.1932374238967896, 5.19229257106781, 5.191354751586914, 5.190423011779785, 5.189496159553528, 5.1885727643966675, 5.187652349472046, 5.186733603477478, 5.185815691947937, 5.184897780418396, 5.1839786767959595, 5.183057188987732, 5.182133197784424, 5.18120551109314, 5.180272579193115, 5.179334759712219, 5.1783905029296875, 5.1774386167526245, 5.176479339599609, 5.175510287284851, 5.174532055854797, 5.173542857170105, 5.172542452812195, 5.171529054641724, 5.1705029010772705, 5.169462561607361, 5.168406844139099, 5.167335271835327, 5.16624653339386, 5.165139675140381, 5.164013862609863, 5.16286838054657, 5.1617008447647095, 5.16051185131073, 5.15929913520813, 5.158061861991882, 5.156798839569092, 5.155508756637573, 5.154190540313721, 5.152842044830322, 5.15146279335022, 5.1500513553619385, 5.148605585098267, 5.147124528884888, 5.145605802536011, 5.144048571586609, 5.142450213432312, 5.140809893608093, 5.139124870300293, 5.137393832206726, 5.1356141567230225, 5.133784055709839, 5.131901144981384, 5.129963397979736, 5.127967715263367, 5.12591278553009, 5.123794913291931, 5.121611714363098, 5.119360327720642, 5.117038369178772, 5.114642143249512, 5.11216938495636, 5.109615683555603, 5.106978416442871, 5.104254126548767, 5.101438760757446, 5.09852921962738, 5.095521450042725, 5.092411398887634, 5.089195251464844, 5.085868000984192, 5.082427144050598, 5.078866958618164, 5.075182557106018, 5.071370959281921, 5.067427277565002, 5.063345670700073, 5.059122562408447, 5.054753303527832, 5.050232768058777, 5.045556306838989, 5.040719270706177, 5.035717248916626, 5.0305458307266235, 5.025200963020325, 5.019678235054016, 5.013973355293274, 5.008083462715149, 5.002005219459534, 4.995734930038452, 4.989270567893982, 4.982610106468201, 4.975751757621765, 4.968695402145386, 4.961439728736877, 4.953985810279846, 4.946334958076477, 4.9384894371032715, 4.9304516315460205, 4.922225713729858, 4.913817286491394, 4.905231475830078, 4.896475791931152, 4.887557864189148, 4.8784871101379395, 4.869272947311401, 4.859926700592041, 4.850460767745972, 4.840887069702148, 4.831219673156738, 4.821471452713013, 4.811658024787903, 4.801793098449707, 4.7918922901153564, 4.781970381736755, 4.772041440010071, 4.762120485305786, 4.752221703529358, 4.742358684539795, 4.7325440645217896, 4.722791075706482, 4.7131108045578, 4.703514456748962, 4.694012522697449, 4.684614181518555, 4.675328612327576, 4.66616415977478, 4.657128214836121, 4.648228764533997, 4.639470338821411, 4.630861163139343, 4.622405171394348, 4.614107012748718, 4.605972409248352]\n",
            "{'DET': 0, 'NN': 1, 'V': 2, 'ADV': 3}\n",
            "['The', 'dog', 'ate', 'the', 'apple']\n",
            "['DET', 'NN', 'V', 'DET', 'NN']\n",
            "[0, 1, 0, 0, 1]\n",
            "--------------\n",
            "{'DET': 0, 'NN': 1, 'V': 2, 'ADV': 3}\n",
            "['Jane', 'walked', 'into', 'the', 'room']\n",
            "['NN', 'V', 'DET', 'DET', 'NN']\n",
            "[1, 1, 0, 0, 1]\n",
            "--------------\n",
            "{'DET': 0, 'NN': 1, 'V': 2, 'ADV': 3}\n",
            "['Everybody', 'does', 'machine', 'learning', 'nowadays']\n",
            "['NN', 'V', 'NN', 'NN', 'ADV']\n",
            "[0, 1, 1, 1, 1]\n",
            "--------------\n",
            "{'DET': 0, 'NN': 1, 'V': 2, 'ADV': 3}\n",
            "['It', 'was', 'late', 'in', 'the', 'day', 'and', 'everyone', 'was', 'walking', 'home', 'after', 'a', 'long', 'day', 'at', 'work']\n",
            "['NN', 'V', 'ADV', 'DET', 'DET', 'NN', 'DET', 'NN', 'V', 'V', 'NN', 'ADV', 'DET', 'DET', 'NN', 'DET', 'NN']\n",
            "[1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0]\n",
            "--------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = torch.FloatTensor(losses_gru)\n",
        "perplexity  = torch.exp(b)\n",
        "print(\"PP:\", perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2aaD4kDpOZM",
        "outputId": "d3fd42ce-f59b-4a09-9796-bc249d81d3ab"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP: tensor([232.1425, 229.7848, 227.5004, 225.2883, 223.1482, 221.0794, 219.0820,\n",
            "        217.1562, 215.3019, 213.5189, 211.8071, 210.1662, 208.5957, 207.0949,\n",
            "        205.6628, 204.2982, 202.9996, 201.7657, 200.5942, 199.4833, 198.4308,\n",
            "        197.4345, 196.4919, 195.6005, 194.7581, 193.9620, 193.2096, 192.4988,\n",
            "        191.8270, 191.1919, 190.5913, 190.0231, 189.4850, 188.9755, 188.4922,\n",
            "        188.0336, 187.5980, 187.1841, 186.7899, 186.4142, 186.0559, 185.7137,\n",
            "        185.3863, 185.0727, 184.7721, 184.4834, 184.2057, 183.9383, 183.6805,\n",
            "        183.4314, 183.1905, 182.9571, 182.7308, 182.5109, 182.2970, 182.0885,\n",
            "        181.8851, 181.6864, 181.4919, 181.3013, 181.1143, 180.9306, 180.7498,\n",
            "        180.5716, 180.3959, 180.2223, 180.0505, 179.8805, 179.7119, 179.5445,\n",
            "        179.3781, 179.2126, 179.0478, 178.8833, 178.7192, 178.5552, 178.3911,\n",
            "        178.2269, 178.0623, 177.8972, 177.7312, 177.5646, 177.3971, 177.2283,\n",
            "        177.0583, 176.8869, 176.7139, 176.5392, 176.3627, 176.1840, 176.0033,\n",
            "        175.8203, 175.6348, 175.4467, 175.2558, 175.0619, 174.8649, 174.6648,\n",
            "        174.4609, 174.2536, 174.0424, 173.8273, 173.6078, 173.3840, 173.1556,\n",
            "        172.9222, 172.6839, 172.4403, 172.1912, 171.9363, 171.6755, 171.4083,\n",
            "        171.1346, 170.8541, 170.5664, 170.2715, 169.9687, 169.6579, 169.3388,\n",
            "        169.0109, 168.6740, 168.3277, 167.9716, 167.6053, 167.2284, 166.8405,\n",
            "        166.4412, 166.0301, 165.6067, 165.1705, 164.7212, 164.2580, 163.7809,\n",
            "        163.2890, 162.7819, 162.2592, 161.7202, 161.1647, 160.5920, 160.0014,\n",
            "        159.3927, 158.7653, 158.1187, 157.4523, 156.7659, 156.0588, 155.3306,\n",
            "        154.5811, 153.8098, 153.0165, 152.2008, 151.3626, 150.5015, 149.6177,\n",
            "        148.7110, 147.7816, 146.8293, 145.8546, 144.8577, 143.8391, 142.7992,\n",
            "        141.7388, 140.6585, 139.5593, 138.4420, 137.3079, 136.1582, 134.9942,\n",
            "        133.8174, 132.6293, 131.4317, 130.2262, 129.0147, 127.7993, 126.5816,\n",
            "        125.3638, 124.1476, 122.9353, 121.7285, 120.5292, 119.3393, 118.1602,\n",
            "        116.9937, 115.8414, 114.7044, 113.5841, 112.4818, 111.3982, 110.3343,\n",
            "        109.2908, 108.2685, 107.2678, 106.2893, 105.3332, 104.3999, 103.4895,\n",
            "        102.6024, 101.7384, 100.8977, 100.0802])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KXFLCMN_pLDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KlKi0HRepLGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6QEfMRDbpFWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sMg-BnzgpFZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lATwaLlMpFdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8fJ2JdGeo_QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ja7hxUOgo1FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Wr5TV8M-o1Ie"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}