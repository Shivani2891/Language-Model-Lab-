{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9KgzkSr6vwO"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the libraries and define Transformer Model"
      ],
      "metadata": {
        "id": "iORwKmATizjn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLBq3JqT6vwX"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            device = src.device\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            self.src_mask = mask\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnf0Izla6vwZ"
      },
      "source": [
        "``PositionalEncoding`` module injects some information about the\n",
        "relative or absolute position of the tokens in the sequence.  Here, we use ``sine`` and ``cosine`` functions of\n",
        "different frequencies.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZcv-QP76vwa"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljZkOLUh6vwc"
      },
      "source": [
        "Load and batch data\n",
        "-------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AndQvwA26vwc"
      },
      "source": [
        "The training process uses Wikitext-2 dataset from ``torchtext``. The\n",
        "vocab object is built based on the train dataset and is used to numericalize\n",
        "tokens into tensors. Starting from sequential data, the ``batchify()``\n",
        "function arranges the dataset into columns, trimming off any tokens remaining\n",
        "after the data has been divided into batches of size ``batch_size``.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR_bqvLv6vwd"
      },
      "outputs": [],
      "source": [
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "TEXT = torchtext.legacy.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
        "                            init_token='<sos>',\n",
        "                            eos_token='<eos>',\n",
        "                            lower=True)\n",
        "train_txt, val_txt, test_txt = torchtext.legacy.datasets.WikiText2.splits(TEXT)\n",
        "TEXT.build_vocab(train_txt)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    data = TEXT.numericalize([data.examples[0].text])\n",
        "    # Divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_txt, batch_size)\n",
        "val_data = batchify(val_txt, eval_batch_size)\n",
        "test_data = batchify(test_txt, eval_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM4GvkJu6vwd"
      },
      "source": [
        "Functions to generate input and target sequence\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRgKHgdU6vwe"
      },
      "source": [
        "``get_batch()`` function generates the input and target sequence for\n",
        "the transformer model. It subdivides the source data into chunks of\n",
        "length ``bptt``. For the language modeling task, the model needs the\n",
        "following words as ``Target``. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxGauH3f6vwe"
      },
      "outputs": [],
      "source": [
        "bptt = 35\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcTjli_w6vwe"
      },
      "source": [
        "Initiate an instance\n",
        "--------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrq2IKtU6vwf"
      },
      "source": [
        "The model is set up with the hyperparameter below. The vocab size is\n",
        "equal to the length of the vocab object.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdV0WF2k6vwf"
      },
      "outputs": [],
      "source": [
        "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
        "emsize = 200 # embedding dimension\n",
        "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2 # the number of heads in the multiheadattention models\n",
        "dropout = 0.2 # the dropout value\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4Dv6yKn6vwf"
      },
      "source": [
        "Run the model\n",
        "-------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "532xntvI6vwf"
      },
      "source": [
        "CrossEntropyLoss is applied to track the loss and SGD implements stochastic gradient descent method as the optimizer. The initial learning rate is set to 5.0. StepLR is applied to adjust the learn rate through epochs. During the training, we use nn.utils.clip_grad_norm function to scale all the gradient together to prevent exploding.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3siGiRI26vwg"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "def train():\n",
        "    model.train() # Turn on the train mode\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 200\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output = eval_model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufm8KWG16vwg"
      },
      "source": [
        "Loop over epochs. Save the model if the validation loss is the best.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Knu2Ak9u6vwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c70c852-edcf-4dba-81e1-762b1d376708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:370: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/ 2981 batches | lr 5.00 | ms/batch 655.68 | loss  8.04 | ppl  3116.92\n",
            "| epoch   1 |   400/ 2981 batches | lr 5.00 | ms/batch 651.89 | loss  6.78 | ppl   881.13\n",
            "| epoch   1 |   600/ 2981 batches | lr 5.00 | ms/batch 654.70 | loss  6.36 | ppl   578.55\n",
            "| epoch   1 |   800/ 2981 batches | lr 5.00 | ms/batch 649.75 | loss  6.23 | ppl   507.23\n",
            "| epoch   1 |  1000/ 2981 batches | lr 5.00 | ms/batch 649.40 | loss  6.12 | ppl   453.14\n",
            "| epoch   1 |  1200/ 2981 batches | lr 5.00 | ms/batch 654.01 | loss  6.08 | ppl   438.99\n",
            "| epoch   1 |  1400/ 2981 batches | lr 5.00 | ms/batch 656.49 | loss  6.04 | ppl   421.47\n",
            "| epoch   1 |  1600/ 2981 batches | lr 5.00 | ms/batch 664.18 | loss  6.05 | ppl   426.14\n",
            "| epoch   1 |  1800/ 2981 batches | lr 5.00 | ms/batch 669.43 | loss  5.96 | ppl   386.40\n",
            "| epoch   1 |  2000/ 2981 batches | lr 5.00 | ms/batch 678.18 | loss  5.95 | ppl   385.64\n",
            "| epoch   1 |  2200/ 2981 batches | lr 5.00 | ms/batch 688.76 | loss  5.85 | ppl   346.80\n",
            "| epoch   1 |  2400/ 2981 batches | lr 5.00 | ms/batch 697.69 | loss  5.90 | ppl   363.59\n",
            "| epoch   1 |  2600/ 2981 batches | lr 5.00 | ms/batch 712.25 | loss  5.90 | ppl   363.47\n",
            "| epoch   1 |  2800/ 2981 batches | lr 5.00 | ms/batch 726.28 | loss  5.80 | ppl   330.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 2081.99s | valid loss  5.69 | valid ppl   297.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2981 batches | lr 4.51 | ms/batch 748.68 | loss  5.80 | ppl   331.81\n",
            "| epoch   2 |   400/ 2981 batches | lr 4.51 | ms/batch 745.03 | loss  5.77 | ppl   320.25\n",
            "| epoch   2 |   600/ 2981 batches | lr 4.51 | ms/batch 752.46 | loss  5.61 | ppl   274.07\n",
            "| epoch   2 |   800/ 2981 batches | lr 4.51 | ms/batch 756.50 | loss  5.64 | ppl   281.52\n",
            "| epoch   2 |  1000/ 2981 batches | lr 4.51 | ms/batch 752.91 | loss  5.59 | ppl   267.91\n",
            "| epoch   2 |  1200/ 2981 batches | lr 4.51 | ms/batch 743.06 | loss  5.62 | ppl   274.93\n",
            "| epoch   2 |  1400/ 2981 batches | lr 4.51 | ms/batch 741.26 | loss  5.64 | ppl   280.18\n",
            "| epoch   2 |  1600/ 2981 batches | lr 4.51 | ms/batch 748.73 | loss  5.67 | ppl   289.81\n",
            "| epoch   2 |  1800/ 2981 batches | lr 4.51 | ms/batch 738.60 | loss  5.60 | ppl   269.36\n",
            "| epoch   2 |  2000/ 2981 batches | lr 4.51 | ms/batch 737.83 | loss  5.62 | ppl   276.97\n",
            "| epoch   2 |  2200/ 2981 batches | lr 4.51 | ms/batch 740.33 | loss  5.52 | ppl   249.36\n",
            "| epoch   2 |  2400/ 2981 batches | lr 4.51 | ms/batch 738.00 | loss  5.58 | ppl   264.77\n",
            "| epoch   2 |  2600/ 2981 batches | lr 4.51 | ms/batch 738.77 | loss  5.60 | ppl   269.69\n",
            "| epoch   2 |  2800/ 2981 batches | lr 4.51 | ms/batch 735.53 | loss  5.52 | ppl   248.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 2283.13s | valid loss  5.64 | valid ppl   280.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2981 batches | lr 4.29 | ms/batch 730.13 | loss  5.55 | ppl   257.89\n",
            "| epoch   3 |   400/ 2981 batches | lr 4.29 | ms/batch 726.33 | loss  5.55 | ppl   257.97\n",
            "| epoch   3 |   600/ 2981 batches | lr 4.29 | ms/batch 717.71 | loss  5.37 | ppl   214.13\n",
            "| epoch   3 |   800/ 2981 batches | lr 4.29 | ms/batch 717.76 | loss  5.42 | ppl   225.44\n",
            "| epoch   3 |  1000/ 2981 batches | lr 4.29 | ms/batch 716.38 | loss  5.38 | ppl   217.11\n",
            "| epoch   3 |  1200/ 2981 batches | lr 4.29 | ms/batch 722.36 | loss  5.41 | ppl   223.08\n",
            "| epoch   3 |  1400/ 2981 batches | lr 4.29 | ms/batch 720.08 | loss  5.43 | ppl   228.61\n",
            "| epoch   3 |  1600/ 2981 batches | lr 4.29 | ms/batch 717.64 | loss  5.47 | ppl   237.65\n",
            "| epoch   3 |  1800/ 2981 batches | lr 4.29 | ms/batch 710.02 | loss  5.40 | ppl   222.15\n",
            "| epoch   3 |  2000/ 2981 batches | lr 4.29 | ms/batch 711.60 | loss  5.43 | ppl   227.21\n",
            "| epoch   3 |  2200/ 2981 batches | lr 4.29 | ms/batch 708.97 | loss  5.32 | ppl   204.78\n",
            "| epoch   3 |  2400/ 2981 batches | lr 4.29 | ms/batch 705.33 | loss  5.39 | ppl   219.85\n",
            "| epoch   3 |  2600/ 2981 batches | lr 4.29 | ms/batch 703.60 | loss  5.41 | ppl   224.34\n",
            "| epoch   3 |  2800/ 2981 batches | lr 4.29 | ms/batch 701.36 | loss  5.35 | ppl   210.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 2195.50s | valid loss  5.50 | valid ppl   243.48\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "epochs = 3 # The number of epochs\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyRvyQtb6vwg"
      },
      "source": [
        "Evaluate the model with the test dataset\n",
        "-------------------------------------\n",
        "\n",
        "Apply the best model to check the result with the test dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFn_pBGj6vwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b727b9ca-2168-4adc-9068-fe07d2a66a25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.40 | test ppl   221.77\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": " pytorch transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}