{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 3.LSTM Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNpb2U2Zm5RO"
      },
      "source": [
        "## Text Classification ##\n",
        "\n",
        "This notebook is based on this Pytorch tutorial: https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial\n",
        "        \n",
        "In this notebook, we will train a network to learn how to classify the country of a name. In this example, we will tokenize per character rather than per word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBE-2hVCm5RR"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import unicodedata\n",
        "import string\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTt52MPzm5RT"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqGVlg-Lm5RU",
        "outputId": "933f55a6-79b1-46de-c5a6-67293130ea7f"
      },
      "source": [
        "# Vocabulary\n",
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "print(n_letters)\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "print(unicodeToAscii('Ślusàrski'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57\n",
            "Slusarski\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLYZ5RMCm5RV",
        "outputId": "c5de7266-7d1f-4b41-b878-7920dfa6a680"
      },
      "source": [
        "def findFiles(path): return glob.glob(path)\n",
        "\n",
        "print(findFiles('data/names/*.txt'))\n",
        "\n",
        "# Build the category_lines dictionary, a list of names per language\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "\n",
        "# Read a file and split into lines\n",
        "def readLines(filename):\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicodeToAscii(line) for line in lines]\n",
        "\n",
        "for filename in findFiles('data/names/*.txt'):\n",
        "    category = os.path.splitext(os.path.basename(filename))[0]\n",
        "    all_categories.append(category)\n",
        "    lines = readLines(filename)\n",
        "    category_lines[category] = lines\n",
        "\n",
        "n_categories = len(all_categories)\n",
        "print(n_categories) # Classes\n",
        "# print(category_lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "zxHuaxsUm5RW",
        "outputId": "37a0c175-25ef-486d-efea-00c07a72a3a8"
      },
      "source": [
        "print(category_lines['Japanese'][:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a53fe1538ee9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_lines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Japanese'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'Japanese'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R29-D3Ham5RX"
      },
      "source": [
        "### Make everything into one-hot vector ###\n",
        "\n",
        "For it to be used (transform into a tensor)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbM3N1z_m5RY"
      },
      "source": [
        "# Find letter index from all_letters, e.g. \"a\" = 0\n",
        "def letterToIndex(letter):\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
        "def letterToTensor(letter):\n",
        "    tensor = torch.zeros(1, n_letters)\n",
        "    tensor[0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>,\n",
        "# or an array of one-hot letter vectors\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "print(letterToTensor('J'))\n",
        "print(lineToTensor('Jones').size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6toGaKq8m5Ra"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.hidden_comp = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.output_comp = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.output2output = nn.Linear(hidden_size, output_size)\n",
        "        self.tanh_act = nn.Tanh()\n",
        "        self.lsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "#         print(x.shape)\n",
        "        # output, hidden= self.rnn(x,hidden)\n",
        "        combined = torch.cat((x, hidden), 1)\n",
        "        hidden = self.hidden_comp(combined)\n",
        "        hidden = self.tanh_act(hidden)\n",
        "#         print(hidden.shape)\n",
        "        \n",
        "        output = self.output_comp(combined)\n",
        "        output = self.tanh_act(output)\n",
        "#         print(output.shape)\n",
        "        output = self.output2output(output)\n",
        "        output = self.lsoftmax(output)\n",
        "#         print(output.shape)\n",
        "        \n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n",
        "\n",
        "n_hidden = 128\n",
        "rnn = RNN(n_letters, n_hidden, n_categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bF-FTvK6m5Rc"
      },
      "source": [
        "To run a step of this network we need to pass an input (in our case, the Tensor for the current letter) and a previous hidden state (which we initialize as zeros at first). We’ll get back the output (probability of each language) and a next hidden state (which we keep for the next step)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8oCfNiCm5Rc"
      },
      "source": [
        "x = letterToTensor('A')\n",
        "hidden =torch.zeros(1, n_hidden)\n",
        "hidden = rnn.initHidden()\n",
        "\n",
        "output, next_hidden = rnn(x, hidden)\n",
        "print(output.argmax(-1))\n",
        "print(output.topk(3).indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTlAg4xVm5Rd"
      },
      "source": [
        "x = lineToTensor('Albert')\n",
        "hidden = torch.zeros(1, n_hidden)\n",
        "\n",
        "output, next_hidden = rnn(x[0], hidden)\n",
        "print(output.argmax(-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw04Is7-m5Rd"
      },
      "source": [
        "## Preparing for training! ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2RRopJtm5Re"
      },
      "source": [
        "# Helper function to get the actual category from the best\n",
        "\n",
        "def categoryFromOutput(output):\n",
        "    top_n, top_i = output.topk(1)\n",
        "    category_i = top_i[0].item()\n",
        "    return all_categories[category_i], category_i\n",
        "\n",
        "print(categoryFromOutput(output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap5aCSXjm5Re"
      },
      "source": [
        "### Quick way to get training example (Src = Name, Trg = Language) ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNV9eJPAm5Rf"
      },
      "source": [
        "def randomChoice(l):\n",
        "    return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "def randomTrainingExample():\n",
        "    category = randomChoice(all_categories)\n",
        "    line = randomChoice(category_lines[category])\n",
        "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
        "    line_tensor = lineToTensor(line)\n",
        "    return category, line, category_tensor, line_tensor\n",
        "\n",
        "for i in range(10):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    print('category =', category, '/ line =', line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzegN4LYm5Rf"
      },
      "source": [
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx2Saf6_m5Rf"
      },
      "source": [
        "## Train now!!! ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncVzTOxjm5Rg"
      },
      "source": [
        "def train(category_tensor, line_tensor, optim, criterion, model):\n",
        "    \n",
        "    model.train()\n",
        "    hidden = model.initHidden()\n",
        "    model.zero_grad()\n",
        "    \n",
        "    category_tensor = category_tensor.to(device)\n",
        "    line_tensor = line_tensor.to(device)\n",
        "\n",
        "    # Iterate through the whole line!\n",
        "    for i in range(line_tensor.size()[0]):\n",
        "        hidden = hidden.to(device)\n",
        "        output, hidden = model(line_tensor[i], hidden)\n",
        "\n",
        "    loss = criterion(output, category_tensor)\n",
        "    \n",
        "    # zero the parameter gradients\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    return output, loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3V2s3Ntm5Rg"
      },
      "source": [
        "#### Train with the RNN from scrath ####"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSNI6dfhm5Rg"
      },
      "source": [
        "criterion = nn.NLLLoss() # Because we are using Log_softmax\n",
        "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
        "N_HIDDEN = 128\n",
        "\n",
        "model_basic = RNN(n_letters, N_HIDDEN, n_categories) #Vocab_size, hidden size, output classes\n",
        "model_basic = model_basic.to(device)\n",
        "optimizer = optim.SGD(model_basic.parameters(), lr=learning_rate)\n",
        "\n",
        "n_iters = 10000\n",
        "print_every = 5000\n",
        "plot_every = 1000\n",
        "\n",
        "# Keep track of losses for plotting\n",
        "current_loss = 0\n",
        "all_losses = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    output, loss = train(category_tensor, line_tensor, optimizer, criterion, model_basic)\n",
        "    current_loss += loss\n",
        "\n",
        "    # Print iter number, loss, name and guess\n",
        "    if iter % print_every == 0:\n",
        "        guess, guess_i = categoryFromOutput(output)\n",
        "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
        "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
        "\n",
        "    # Add current loss avg to list of losses\n",
        "    if iter % plot_every == 0:\n",
        "        all_losses.append(current_loss / plot_every)\n",
        "        current_loss = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2C8-yg-m5Rg"
      },
      "source": [
        "# PLOT THE RESULT\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AZn8jKJm5Rh"
      },
      "source": [
        "## Evaluate the results with a confusion matrix! ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wVQyvTRm5Rh"
      },
      "source": [
        "# Keep track of correct guesses in a confusion matrix\n",
        "confusion = torch.zeros(n_categories, n_categories)\n",
        "n_confusion = 20000\n",
        "\n",
        "# Just return an output given a line\n",
        "def evaluate(line_tensor, model):\n",
        "    model.eval()\n",
        "    hidden = model.initHidden()\n",
        "    line_tensor = line_tensor.to(device)\n",
        "\n",
        "    for i in range(line_tensor.size()[0]):\n",
        "        hidden = hidden.to(device)\n",
        "        output, hidden = model(line_tensor[i], hidden)\n",
        "\n",
        "    return output\n",
        "\n",
        "# Go through a bunch of examples and record which are correctly guessed\n",
        "for i in range(n_confusion):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    output = evaluate(line_tensor, model_basic)\n",
        "    guess, guess_i = categoryFromOutput(output)\n",
        "    category_i = all_categories.index(category)\n",
        "    confusion[category_i][guess_i] += 1\n",
        "\n",
        "# Normalize by dividing every row by its sum\n",
        "for i in range(n_categories):\n",
        "    confusion[i] = confusion[i] / confusion[i].sum()\n",
        "\n",
        "# Set up plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(confusion.numpy())\n",
        "fig.colorbar(cax)\n",
        "\n",
        "# Set up axes\n",
        "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
        "ax.set_yticklabels([''] + all_categories)\n",
        "\n",
        "# Force label at every tick\n",
        "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "# sphinx_gallery_thumbnail_number = 2\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrMuZVU6m5Rh"
      },
      "source": [
        "## Now we can try predicting our own things! ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym0-D39Im5Rh"
      },
      "source": [
        "def predict(input_line, n_predictions=3):\n",
        "    print('\\n> %s' % input_line)\n",
        "    with torch.no_grad():\n",
        "        output = evaluate(lineToTensor(input_line), model_basic)\n",
        "\n",
        "        # Get top N categories\n",
        "        topv, topi = output.topk(n_predictions, 1, True)\n",
        "        predictions = []\n",
        "\n",
        "        for i in range(n_predictions):\n",
        "            value = topv[0][i].item()\n",
        "            category_index = topi[0][i].item()\n",
        "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
        "            predictions.append([value, all_categories[category_index]])\n",
        "\n",
        "predict('Dovesky')\n",
        "predict('Jackson')\n",
        "predict('Satoshi')\n",
        "predict('Nero')\n",
        "predict('Vivek')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfOTWNBZm5Ri"
      },
      "source": [
        "### So now lets shift to using the RNN Module from Pytorch ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpYbgMWVm5Ri"
      },
      "source": [
        "def categoryToTensor(category):\n",
        "#     tensor = torch.zeros([1, 1], dtype=torch.long)\n",
        "#     tensor[0][0] = all_categories.index(category)\n",
        "    tensor = torch.zeros([1, len(all_categories)], dtype=torch.long)\n",
        "    tensor[0][all_categories.index(category)] = 1\n",
        "    return tensor\n",
        "\n",
        "def randomChoice(l):\n",
        "    return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "def randomTrainingExample():\n",
        "    category = randomChoice(all_categories)\n",
        "    line = randomChoice(category_lines[category])\n",
        "    category_tensor = categoryToTensor(category)\n",
        "    line_tensor = lineToTensor(line)\n",
        "    return category, line, category_tensor, line_tensor\n",
        "\n",
        "for i in range(10):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    print('category =', category, '/ line =', line)\n",
        "#     print(category_tensor.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EAMLEewm5Ri"
      },
      "source": [
        "class RNNPytorch(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNNPytorch, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=1, batch_first=True) # 1 is num_layers\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.lsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "\n",
        "        hidden, output = self.rnn(x, hidden)\n",
        "\n",
        "        output = output.contiguous().view(-1, self.hidden_size)\n",
        "        output = self.fc(output)\n",
        "        output = self.lsoftmax(output)\n",
        "        \n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "        return torch.zeros(1, 1, self.hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWSf9H_Vm5Ri"
      },
      "source": [
        "def train_pytorch_model(category_tensor, line_tensor, optim, criterion, model):\n",
        "    \n",
        "    model.train()\n",
        "    hidden = model.initHidden(line_tensor.shape[1])\n",
        "    model.zero_grad()\n",
        "    line_tensor = line_tensor.permute(1, 0, 2)\n",
        "#     print(line_tensor.shape)\n",
        "    \n",
        "#     print(category_tensor.shape)\n",
        "    category_tensor = category_tensor.to(device)\n",
        "    line_tensor = line_tensor.to(device)\n",
        "    \n",
        "    # Iterate through the whole line!\n",
        "    hidden = hidden.to(device)\n",
        "    output, hidden = model(line_tensor, hidden)\n",
        "\n",
        "    loss = criterion(output, torch.topk(category_tensor, 1).indices.view(-1))\n",
        "    \n",
        "    # zero the parameter gradients\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    return output, loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJIdrXeAm5Rj"
      },
      "source": [
        "#### Train from the RNN using pytorch approach ####"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ds1KNAsm5Rj"
      },
      "source": [
        "criterion = nn.NLLLoss() # Because we are using Log_softmax\n",
        "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
        "N_HIDDEN = 128\n",
        "\n",
        "model_pytorch = RNNPytorch(n_letters, N_HIDDEN, n_categories) #Vocab_size, hidden size, output classes\n",
        "model_pytorch = model_pytorch.to(device)\n",
        "optimizer = optim.SGD(model_pytorch.parameters(), lr=learning_rate)\n",
        "\n",
        "n_iters = 10000\n",
        "print_every = 5000\n",
        "plot_every = 1000\n",
        "\n",
        "# Keep track of losses for plotting\n",
        "current_loss = 0\n",
        "all_losses = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    output, loss = train_pytorch_model(category_tensor, line_tensor, optimizer, criterion, model_pytorch)\n",
        "    current_loss += loss\n",
        "    # Print iter number, loss, name and guess\n",
        "    if iter % print_every == 0:\n",
        "        guess, guess_i = categoryFromOutput(output)\n",
        "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
        "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
        "\n",
        "    # Add current loss avg to list of losses\n",
        "    if iter % plot_every == 0:\n",
        "        all_losses.append(current_loss / plot_every)\n",
        "        current_loss = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFISG1CPm5Rj"
      },
      "source": [
        "# PLOT THE RESULT\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi43O7K6m5Rk"
      },
      "source": [
        "# Just return an output given a line\n",
        "def evaluate(line_tensor, model):\n",
        "    model.eval()\n",
        "    hidden = model.initHidden(line_tensor.shape[1])\n",
        "    hidden = hidden.to(device)\n",
        "\n",
        "    line_tensor = line_tensor.permute(1, 0, 2)\n",
        "    line_tensor = line_tensor.to(device)\n",
        "    output, hidden = model(line_tensor, hidden)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnBa3HiXm5Rk"
      },
      "source": [
        "def predict(input_line, n_predictions=3):\n",
        "    print('\\n> %s' % input_line)\n",
        "    with torch.no_grad():\n",
        "        output = evaluate(lineToTensor(input_line), model_pytorch)\n",
        "\n",
        "        # Get top N categories\n",
        "        topv, topi = output.topk(n_predictions, 1, True)\n",
        "        predictions = []\n",
        "\n",
        "        for i in range(n_predictions):\n",
        "            value = topv[0][i].item()\n",
        "            category_index = topi[0][i].item()\n",
        "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
        "            predictions.append([value, all_categories[category_index]])\n",
        "\n",
        "predict('Dovesky')\n",
        "predict('Jackson')\n",
        "predict('Satoshi')\n",
        "predict('Vivek')\n",
        "predict('Nero')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ATOVoBfm5Rl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-Z2TLpQm5Rl"
      },
      "source": [
        "## Now one of your assigments is to adapt/expand this text classifier! ##\n",
        "\n",
        "- Adapt the task to another similar task\n",
        "- Use LSTMs or GRU to compare with both Vanilla RNN approaches\n",
        "\n",
        "You can keep it at character-level or you can extend it to word-level (the vocabulary will change)\n",
        "\n",
        "Try with different datasets with a similar Input/Output system:\n",
        "- Book or character name -> writer\n",
        "- Game's titles -> company or genre\n",
        "- Any word -> language\n",
        "- Whatever you can think it can be classified"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08sLZDhrdfHP"
      },
      "source": [
        "## Classification of true and fake news using LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLk6gZBum5Rl",
        "outputId": "b1e556cf-30c1-4139-af90-3ba8a84562f7"
      },
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import torch\n",
        "\n",
        "# Models\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# Training\n",
        "import torch.optim as optim\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDK7QwnDnxGV",
        "outputId": "db382506-cd9a-4a7a-d6a3-615683d8b598"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI2heO7FdPpv"
      },
      "source": [
        "##Reading data from google drive\n",
        "\n",
        "*   The data of fake news and true news is downloaded from kaggle in the form of two separate csv files.\n",
        "*   Later the datasets are imported in google drive to be mounted on colab\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "NPaTBY8hgzXG",
        "outputId": "0a27084b-1eb5-4834-a173-e2882c674658"
      },
      "source": [
        "true = pd.read_csv('/gdrive/MyDrive/True.csv')\n",
        "true.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
              "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>U.S. military to accept transgender recruits o...</td>\n",
              "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
              "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
              "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 30, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
              "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...                date\n",
              "0  As U.S. budget fight looms, Republicans flip t...  ...  December 31, 2017 \n",
              "1  U.S. military to accept transgender recruits o...  ...  December 29, 2017 \n",
              "2  Senior U.S. Republican senator: 'Let Mr. Muell...  ...  December 31, 2017 \n",
              "3  FBI Russia probe helped by Australian diplomat...  ...  December 30, 2017 \n",
              "4  Trump wants Postal Service to charge 'much mor...  ...  December 29, 2017 \n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "U1n8oUgun3yP",
        "outputId": "39f06507-e2c6-4286-c4ac-9bebb36c630a"
      },
      "source": [
        "fake = pd.read_csv('/gdrive/MyDrive/Fake.csv')\n",
        "fake.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
              "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
              "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
              "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 30, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
              "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 29, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
              "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 25, 2017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...               date\n",
              "0   Donald Trump Sends Out Embarrassing New Year’...  ...  December 31, 2017\n",
              "1   Drunk Bragging Trump Staffer Started Russian ...  ...  December 31, 2017\n",
              "2   Sheriff David Clarke Becomes An Internet Joke...  ...  December 30, 2017\n",
              "3   Trump Is So Obsessed He Even Has Obama’s Name...  ...  December 29, 2017\n",
              "4   Pope Francis Just Called Out Donald Trump Dur...  ...  December 25, 2017\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppr1C6wYeQCm"
      },
      "source": [
        "The raw datasets have field such as title of the news,text written in the news ,subject along with date the news was published."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04KPGIj6es7w"
      },
      "source": [
        "Now the two separate csv files are joined after the true and fake news are allocated labels ;1 for true and 0 for fake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8m8a26VtoohW",
        "outputId": "5219bc6f-bb06-4a98-f5ef-e1e6703b2fdd"
      },
      "source": [
        "true['truth'] = 1\n",
        "fake['truth'] = 0\n",
        "df = pd.concat([true, fake], axis=0, ignore_index=True)\n",
        "df.shape\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>truth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
              "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>U.S. military to accept transgender recruits o...</td>\n",
              "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
              "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
              "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 30, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
              "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... truth\n",
              "0  As U.S. budget fight looms, Republicans flip t...  ...     1\n",
              "1  U.S. military to accept transgender recruits o...  ...     1\n",
              "2  Senior U.S. Republican senator: 'Let Mr. Muell...  ...     1\n",
              "3  FBI Russia probe helped by Australian diplomat...  ...     1\n",
              "4  Trump wants Postal Service to charge 'much mor...  ...     1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijJEvRs0re4m",
        "outputId": "3920bb2b-fb95-4103-a50f-7840c0cca10a"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzBet9Jce_im"
      },
      "source": [
        "##Preprocessing and Splitting the dataset into training,validation and testing sub-parts\n",
        "\n",
        "*   In the preprocessing part, the rows with missing values are deleted. Also,text strings are trimmed to a specified word limit.Trimming the samples in a dataset is not necessary but with trimmed dataset,training is faster for heavier models and is normally enough to predict the outcome.\n",
        "*Later, the only the features of title of the news and text of the news along with label are kept. Rest are eliminated.\n",
        "*  A new feature joining title and text is created which has string sfrom both the columns to remove the space between both.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Irnsbgi1vkSR"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_test_ratio = 0.10\n",
        "train_valid_ratio = 0.80\n",
        "# Drop rows with empty text\n",
        "df.drop( df[df.text.str.len() < 5].index, inplace=True)\n",
        "first_n_words = 200\n",
        "def trim_string(x):\n",
        "\n",
        "    x = x.split(maxsplit=first_n_words)\n",
        "    x = ' '.join(x[:first_n_words])\n",
        "\n",
        "    return x\n",
        "df['titletext'] = df['title'] + \". \" + df['text']\n",
        "df= df.reindex(columns=['truth', 'title', 'text', 'titletext'])\n",
        "# Trim text and titletext to first_n_words\n",
        "df['text'] = df['text'].apply(trim_string)\n",
        "df['titletext'] = df['titletext'].apply(trim_string) \n",
        "# Split according to label\n",
        "df_real = df[df['truth'] == 0]\n",
        "df_fake = df[df['truth'] == 1]\n",
        "\n",
        "# Train-test split\n",
        "df_real_full_train, df_real_test = train_test_split(df_real, train_size = train_test_ratio, random_state = 1)\n",
        "df_fake_full_train, df_fake_test = train_test_split(df_fake, train_size = train_test_ratio, random_state = 1)\n",
        "\n",
        "# Train-valid split\n",
        "df_real_train, df_real_valid = train_test_split(df_real_full_train, train_size = train_valid_ratio, random_state = 1)\n",
        "df_fake_train, df_fake_valid = train_test_split(df_fake_full_train, train_size = train_valid_ratio, random_state = 1)\n",
        "\n",
        "# Concatenate splits of different labels\n",
        "df_train = pd.concat([df_real_train, df_fake_train], ignore_index=True, sort=False)\n",
        "df_valid = pd.concat([df_real_valid, df_fake_valid], ignore_index=True, sort=False)\n",
        "df_test = pd.concat([df_real_test, df_fake_test], ignore_index=True, sort=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TzOQH9igTrB"
      },
      "source": [
        "This is how the datsets look after preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HtznJfXZZGJt",
        "outputId": "9bb11018-5cc8-44f1-c11a-6c138f8adcea"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>truth</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>titletext</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Sarah Silverman’s Hysterical Video For Bernie...</td>\n",
              "      <td>On Monday, Sarah Silverman released a hilariou...</td>\n",
              "      <td>Sarah Silverman’s Hysterical Video For Bernie ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Perfect Match? Evidence Leans Towards Sarah P...</td>\n",
              "      <td>Donald Trump set tongues wagging with a tweet ...</td>\n",
              "      <td>Perfect Match? Evidence Leans Towards Sarah Pa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>NOT NEWS: [Graphic Video] Michigan Woman Runs ...</td>\n",
              "      <td>This is a story that will never make the news....</td>\n",
              "      <td>NOT NEWS: [Graphic Video] Michigan Woman Runs ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>HEROIC Muslim Student Helps Track Down Man Wh...</td>\n",
              "      <td>The situation that 17-year old Ahmed Khalifa s...</td>\n",
              "      <td>HEROIC Muslim Student Helps Track Down Man Who...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>REVEALED: Trump Illegally Violated Embargo Ag...</td>\n",
              "      <td>The law and order candidate broke the law agai...</td>\n",
              "      <td>REVEALED: Trump Illegally Violated Embargo Aga...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   truth  ...                                          titletext\n",
              "0      0  ...  Sarah Silverman’s Hysterical Video For Bernie ...\n",
              "1      0  ...  Perfect Match? Evidence Leans Towards Sarah Pa...\n",
              "2      0  ...  NOT NEWS: [Graphic Video] Michigan Woman Runs ...\n",
              "3      0  ...  HEROIC Muslim Student Helps Track Down Man Who...\n",
              "4      0  ...  REVEALED: Trump Illegally Violated Embargo Aga...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMjgA4UHgllM"
      },
      "source": [
        "##saving datasets in separate csv files "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X89mVxWBW3J"
      },
      "source": [
        "destination_folder = '/gdrive/MyDrive/'\n",
        "# Write preprocessed data\n",
        "df_train.to_csv(destination_folder + '/train.csv', index=False)\n",
        "df_valid.to_csv(destination_folder + '/valid.csv', index=False)\n",
        "df_test.to_csv(destination_folder + '/test.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-phVMhKJZma",
        "outputId": "8d955f10-b196-4e41-ac20-e2b79524bca2"
      },
      "source": [
        " print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.10.0+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjdVJMVBvQuy"
      },
      "source": [
        "## Further preprocessing, batchifying and creating vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiuGSoYGoyZv"
      },
      "source": [
        "Torchtext.data provides the ability to adapt or change the dataset.Like Field part allows to describe a a feature with particular instructions to convert to tensor , and Dataset defines columns stored in CSV, TSV, or JSON format.BucketIterator defines an iterator that batches examples of similar lengths together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rht76JqgB_1W",
        "outputId": "8df71fec-5cf6-4265-f4d4-8f441581d4d0"
      },
      "source": [
        "!pip install -U torchtext==0.11.0\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.11.0 in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (4.62.3)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchtext==0.11.0) (3.10.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKRA0whVqRDe"
      },
      "source": [
        "\n",
        "\n",
        "*   With Field text data is further preprocessed. Text field strings are tokenized in sequential examples with spacy , and the text is changed to lowercase with lower=true\n",
        "\n",
        "*  With batch_first True, tensors with the batch dimension first are produced.\n",
        "* With include_lengths, tuples of padded minibatch is returned. \n",
        "*  Then with Tabulardataset datasets of training,testing and validating datset are created but with removed first line i.e. without headers.\n",
        "* Bucketiterator batches the examples of equal length in each of the 3 datasets with batch size =32. Sorting examples within the batch and the dataset is true.\n",
        "*The train, valid, and test iterators are created that load the data.\n",
        "*With build_vocab, vocabulary is created from training dataset with minimum frequency for a token to be part of vocab is 3.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txzg2qGXrfFh"
      },
      "source": [
        "\n",
        "source_folder = '/gdrive/MyDrive'\n",
        "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
        "text_field = Field(tokenize='spacy', lower=True, include_lengths=True, batch_first=True)\n",
        "fields = [('truth', label_field), ('title', text_field), ('text', text_field), ('titletext', text_field)]\n",
        "\n",
        "# TabularDataset\n",
        "\n",
        "train, valid, test = TabularDataset.splits(path=source_folder, train='train.csv', validation='valid.csv', test='test.csv',\n",
        "                                           format='CSV', fields=fields, skip_header=True)\n",
        "\n",
        "# Iterators\n",
        "\n",
        "train_iter = BucketIterator(train, batch_size=32, sort_key=lambda x: len(x.text),\n",
        "                            device=device, sort=True, sort_within_batch=True)\n",
        "valid_iter = BucketIterator(valid, batch_size=32, sort_key=lambda x: len(x.text),\n",
        "                            device=device, sort=True, sort_within_batch=True)\n",
        "test_iter = BucketIterator(test, batch_size=32, sort_key=lambda x: len(x.text),\n",
        "                            device=device, sort=True, sort_within_batch=True)\n",
        "\n",
        "# Vocabulary\n",
        "\n",
        "text_field.build_vocab(train, min_freq=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QpuAAXihB4z"
      },
      "source": [
        "## Defining LSTM Model for classification task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgbQCXJ8wRoa"
      },
      "source": [
        "LSTM class is defined that inherits from the nn.Module. Inside the LSTM, we construct an Embedding layer, followed by a bi-LSTM layer, and ending with a fully connected linear layer. In the forward function, we pass the text IDs through the embedding layer to get the embeddings, pass it through the LSTM accommodating variable-length sequences, learn from both directions, pass it through the fully connected linear layer, and finally sigmoid to get the probability of the sequences belonging to FAKE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5LujB8qWU2w"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, dimension=128):\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(len(text_field.vocab), 300)\n",
        "        self.dimension = dimension\n",
        "        self.lstm = nn.LSTM(input_size=300,\n",
        "                            hidden_size=dimension,\n",
        "                            num_layers=1,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "        self.drop = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.fc = nn.Linear(2*dimension, 1)\n",
        "\n",
        "    def forward(self, text, text_len):\n",
        "\n",
        "        text_emb = self.embedding(text)\n",
        "\n",
        "        packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
        "        packed_output, _ = self.lstm(packed_input)\n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n",
        "        out_reverse = output[:, 0, self.dimension:]\n",
        "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
        "        text_fea = self.drop(out_reduced)\n",
        "\n",
        "        text_fea = self.fc(text_fea)\n",
        "        text_fea = torch.squeeze(text_fea, 1)\n",
        "        text_out = torch.sigmoid(text_fea)\n",
        "\n",
        "        return text_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBsROMj_w8sm"
      },
      "source": [
        "Before training, save and load functions are built for checkpoints and metrics. For checkpoints, the model parameters and optimizer are saved; for metrics, the train loss, valid loss, and global steps are saved so diagrams can be easily reconstructed later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMXk0NMGWVC3"
      },
      "source": [
        "# Save and Load Functions\n",
        "\n",
        "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict(),\n",
        "                  'valid_loss': valid_loss}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_checkpoint(load_path, model, optimizer):\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
        "    \n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "\n",
        "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'train_loss_list': train_loss_list,\n",
        "                  'valid_loss_list': valid_loss_list,\n",
        "                  'global_steps_list': global_steps_list}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_metrics(load_path):\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7B74KDQhI4K"
      },
      "source": [
        "## Defining training function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhYLCA8AxAf8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIdXTMqVWVNs"
      },
      "source": [
        "# Training Function\n",
        "\n",
        "def train(model,\n",
        "          optimizer,\n",
        "          criterion = nn.BCELoss(), # binary cross entropy loss\n",
        "          train_loader = train_iter,\n",
        "          valid_loader = valid_iter,\n",
        "          num_epochs = 5,           # these are default epochs\n",
        "          eval_every = len(train_iter) // 2,\n",
        "          file_path = destination_folder,\n",
        "          best_valid_loss = float(\"Inf\")):\n",
        "    \n",
        "    # initialize running values\n",
        "    running_loss = 0.0\n",
        "    valid_running_loss = 0.0\n",
        "    global_step = 0\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    global_steps_list = []\n",
        "\n",
        "    # training loop\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for (labels, (title, title_len), (text, text_len), (titletext, titletext_len)), _ in train_loader:           \n",
        "            labels = labels.to(device)\n",
        "            titletext = titletext.to(device)\n",
        "            titletext_len = titletext_len.to(device)\n",
        "            output = model(titletext, titletext_len)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update running values\n",
        "            running_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "            # evaluation step\n",
        "            if global_step % eval_every == 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():                    \n",
        "                  # validation loop\n",
        "                  for (labels, (title, title_len), (text, text_len), (titletext, titletext_len)), _ in valid_loader:\n",
        "                      labels = labels.to(device)\n",
        "                      titletext = titletext.to(device)\n",
        "                      titletext_len = titletext_len.to(device)\n",
        "                      output = model(titletext, titletext_len)\n",
        "\n",
        "                      loss = criterion(output, labels)\n",
        "                      valid_running_loss += loss.item()\n",
        "\n",
        "                # evaluation\n",
        "                average_train_loss = running_loss / eval_every\n",
        "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
        "                train_loss_list.append(average_train_loss)\n",
        "                valid_loss_list.append(average_valid_loss)\n",
        "                global_steps_list.append(global_step)\n",
        "\n",
        "                # resetting running values\n",
        "                running_loss = 0.0                \n",
        "                valid_running_loss = 0.0\n",
        "                model.train()\n",
        "\n",
        "                # print progress\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
        "                              average_train_loss, average_valid_loss))\n",
        "                # checkpoint\n",
        "                if best_valid_loss > average_valid_loss:\n",
        "                    best_valid_loss = average_valid_loss\n",
        "                    save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)\n",
        "                    save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    \n",
        "    save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    print('Finished Training!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V32JDigJho6U"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKsFsMm0xlLo"
      },
      "source": [
        "The LSTM model is trained with 10 epochs and save the checkpoint and metrics whenever a hyperparameter setting achieves the best (lowest) validation loss. Apart from that, learning rate is small with the value 0.001 and optimizer chosen is Adam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wf8-I0yjXH69",
        "outputId": "2c781add-7017-4dda-9fc8-e3d8800715ef"
      },
      "source": [
        "model = LSTM().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train(model=model, optimizer=optimizer, num_epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [55/1110], Train Loss: 0.5173, Valid Loss: 0.3484\n",
            "Model saved to ==> /gdrive/MyDrive//model.pt\n",
            "Model saved to ==> /gdrive/MyDrive//metrics.pt\n",
            "Epoch [1/10], Step [110/1110], Train Loss: 0.1419, Valid Loss: 0.0403\n",
            "Model saved to ==> /gdrive/MyDrive//model.pt\n",
            "Model saved to ==> /gdrive/MyDrive//metrics.pt\n",
            "Epoch [2/10], Step [165/1110], Train Loss: 0.0352, Valid Loss: 0.0424\n",
            "Epoch [2/10], Step [220/1110], Train Loss: 0.0495, Valid Loss: 0.0288\n",
            "Model saved to ==> /gdrive/MyDrive//model.pt\n",
            "Model saved to ==> /gdrive/MyDrive//metrics.pt\n",
            "Epoch [3/10], Step [275/1110], Train Loss: 0.0156, Valid Loss: 0.0228\n",
            "Model saved to ==> /gdrive/MyDrive//model.pt\n",
            "Model saved to ==> /gdrive/MyDrive//metrics.pt\n",
            "Epoch [3/10], Step [330/1110], Train Loss: 0.0138, Valid Loss: 0.0151\n",
            "Model saved to ==> /gdrive/MyDrive//model.pt\n",
            "Model saved to ==> /gdrive/MyDrive//metrics.pt\n",
            "Epoch [4/10], Step [385/1110], Train Loss: 0.0038, Valid Loss: 0.0188\n",
            "Epoch [4/10], Step [440/1110], Train Loss: 0.0047, Valid Loss: 0.0144\n",
            "Model saved to ==> /gdrive/MyDrive//model.pt\n",
            "Model saved to ==> /gdrive/MyDrive//metrics.pt\n",
            "Epoch [5/10], Step [495/1110], Train Loss: 0.0038, Valid Loss: 0.0145\n",
            "Epoch [5/10], Step [550/1110], Train Loss: 0.0012, Valid Loss: 0.0153\n",
            "Epoch [6/10], Step [605/1110], Train Loss: 0.0023, Valid Loss: 0.0135\n",
            "Model saved to ==> /gdrive/MyDrive//model.pt\n",
            "Model saved to ==> /gdrive/MyDrive//metrics.pt\n",
            "Epoch [6/10], Step [660/1110], Train Loss: 0.0008, Valid Loss: 0.0145\n",
            "Epoch [7/10], Step [715/1110], Train Loss: 0.0008, Valid Loss: 0.0143\n",
            "Epoch [7/10], Step [770/1110], Train Loss: 0.0006, Valid Loss: 0.0150\n",
            "Epoch [8/10], Step [825/1110], Train Loss: 0.0005, Valid Loss: 0.0151\n",
            "Epoch [8/10], Step [880/1110], Train Loss: 0.0004, Valid Loss: 0.0156\n",
            "Epoch [9/10], Step [935/1110], Train Loss: 0.0005, Valid Loss: 0.0144\n",
            "Epoch [9/10], Step [990/1110], Train Loss: 0.0003, Valid Loss: 0.0156\n",
            "Epoch [10/10], Step [1045/1110], Train Loss: 0.0002, Valid Loss: 0.0158\n",
            "Epoch [10/10], Step [1100/1110], Train Loss: 0.0002, Valid Loss: 0.0160\n",
            "Model saved to ==> /gdrive/MyDrive//metrics.pt\n",
            "Finished Training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsgSOJN3hvbV"
      },
      "source": [
        "## Plotting training vs validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "PdK0DgC2XMQt",
        "outputId": "574491c1-936d-4dc8-c966-58fafb89fc41"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\n",
        "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
        "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
        "plt.xlabel('Global Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from <== /gdrive/MyDrive//metrics.pt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v2g7LZviTh-"
      },
      "source": [
        "The graph shows both the training and validation error have followed similar trajectory and are almost nil after the first few iterations. The model is well trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0NFtb0oivEA"
      },
      "source": [
        "## Defining evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLoXb2-LXMe3"
      },
      "source": [
        "# Evaluation Function\n",
        "\n",
        "def evaluate(model, test_loader, version='title', threshold=0.5):\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for (labels, (title, title_len), (text, text_len), (titletext, titletext_len)), _ in test_loader:           \n",
        "            labels = labels.to(device)\n",
        "            titletext = titletext.to(device)\n",
        "            titletext_len = titletext_len.to(device)\n",
        "            output = model(titletext, titletext_len)\n",
        "\n",
        "            output = (output > threshold).int()\n",
        "            y_pred.extend(output.tolist())\n",
        "            y_true.extend(labels.tolist())\n",
        "    \n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
        "    ax= plt.subplot()\n",
        "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
        "\n",
        "    ax.set_title('Confusion Matrix')\n",
        "\n",
        "    ax.set_xlabel('Predicted Labels')\n",
        "    ax.set_ylabel('True Labels')\n",
        "\n",
        "    ax.xaxis.set_ticklabels(['FAKE', 'REAL'])\n",
        "    ax.yaxis.set_ticklabels(['FAKE', 'REAL'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "hkZalzfhXXra",
        "outputId": "31ec7837-5604-49d2-b084-0fbbb4fb27aa"
      },
      "source": [
        "best_model = LSTM().to(device)\n",
        "optimizer = optim.Adam(best_model.parameters(), lr=0.001)\n",
        "\n",
        "load_checkpoint(destination_folder + '/model.pt', best_model, optimizer)\n",
        "evaluate(best_model, test_iter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from <== /gdrive/MyDrive//model.pt\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.9992    0.9961    0.9976     19275\n",
            "           0     0.9964    0.9992    0.9978     20566\n",
            "\n",
            "    accuracy                         0.9977     39841\n",
            "   macro avg     0.9978    0.9977    0.9977     39841\n",
            "weighted avg     0.9977    0.9977    0.9977     39841\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEWCAYAAACHVDePAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwU1bn/8c93QBQVlUVRROMSNHELcY9b3FBw+SHGqMQFVzRRbxI1KtGo0XivMS65XrdgxKAoikEjrkhwiwsKIgF3UTGyKQICAirg8/ujzmAzzvT0DNOzdH/fr1e9puqpqlOncXz6zKlTpxQRmJlZaato6gqYmVnxOdmbmZUBJ3szszLgZG9mVgac7M3MyoCTvZlZGXCyt5Umqa2khyTNk3TfSpRzjKQnGrJuTUHSY5L6NXU9zHI52ZcRST+TNE7S55JmpKS0RwMUfQTQGegYET+tbyERcVdEHNAA9VmBpL0lhaQHqsR/kOJPF1jOpZKG1HZcRPSKiMH1rK5ZUTjZlwlJZwN/Bv6bLDFvDNwE9G6A4r8DvBMRSxugrGKZBfxIUsecWD/gnYa6gDL+f8qaJf9ilgFJawOXAWdExP0RsTAilkTEQxHxm3TMqpL+LGl6Wv4sadW0b29JUyWdI+mT9FfBiWnf74GLgaPSXwwnV20BS9oktaBbp+0TJL0vaYGkDyQdkxN/Lue83SSNTd1DYyXtlrPvaUmXS3o+lfOEpE55/hm+Av4BHJ3ObwUcBdxV5d/qfyV9JGm+pFck7ZniPYHf5nzOf+fU4wpJzwOLgM1S7JS0/2ZJw3PK/6Ok0ZJU8H9AswbgZF8efgSsBjyQ55gLgV2B7sAPgJ2Bi3L2rw+sDWwInAzcKKl9RFxC9tfCvRGxZkTclq8iktYArgd6RUQ7YDdgQjXHdQAeScd2BK4FHqnSMv8ZcCKwHtAGODfftYE7gOPT+oHAa8D0KseMJfs36ADcDdwnabWIeLzK5/xBzjnHAf2BdsCHVco7B9g2fZHtSfZv1y88T4k1Mif78tAR+LSWbpZjgMsi4pOImAX8niyJVVqS9i+JiEeBz4Et61mfr4FtJLWNiBkR8Xo1xxwMvBsRd0bE0ogYCrwFHJpzzO0R8U5ELAaGkSXpGkXEC0AHSVuSJf07qjlmSETMTte8BliV2j/n3yLi9XTOkirlLSL7d7wWGAKcFRFTaynPrME52ZeH2UCnym6UGnRhxVbphym2vIwqXxaLgDXrWpGIWEjWfXI6MEPSI5K+V0B9Kuu0Yc72zHrU507gTGAfqvlLR9K5kt5MXUefkf01k697COCjfDsj4iXgfUBkX0pmjc7Jvjy8CHwJHJbnmOlkN1orbcy3uzgKtRBYPWd7/dydETEyInoAG5C11m8toD6VdZpWzzpVuhP4BfBoanUvl7pZzgOOBNpHxDrAPLIkDVBT10veLhlJZ5D9hTA9lW/W6Jzsy0BEzCO7iXqjpMMkrS5pFUm9JF2VDhsKXCRp3XSj82Kybof6mADsJWnjdHN4QOUOSZ0l9U5991+SdQd9XU0ZjwJbpOGirSUdBWwFPFzPOgEQER8APya7R1FVO2Ap2cid1pIuBtbK2f8xsEldRtxI2gL4A3AsWXfOeZLydjeZFYOTfZlI/c9nk910nUXW9XAm2QgVyBLSOGAiMAkYn2L1udYo4N5U1iusmKArUj2mA3PIEu/PqyljNnAI2Q3O2WQt4kMi4tP61KlK2c9FRHV/tYwEHicbjvkh8AUrdtFUPjA2W9L42q6Tus2GAH+MiH9HxLtkI3rurBzpZNZY5EEBZmalzy17M7My4GRvZlYGnOzNzMqAk72ZWRnI95BNk2q77xW+c2zfMveJ6kZMWrlbrTUrPddQ2x+eWXDOWfzqDS1ubiO37M3MykCzbdmbmTWqEp+durQ/nZlZoSpaFb7kIWkjSU9JekPS65J+meIdJI2S9G762T7FJel6SZMlTZS0fU5Z/dLx7yrn7WeSdpA0KZ1zfSFTZjvZm5kBSIUv+S0FzomIrcimDT9D0lbABcDoiOgGjE7bAL2AbmnpD9ycVUcdgEuAXcimHL+k8gsiHXNqznk9a6uUk72ZGWTdOIUueaRpu8en9QXAm2SztfYGKl9XOZhvJibsDdwRmTHAOpI2IHvnwqiImBMRc4FRQM+0b62IGJPei3AH+Sc5BJzszcwydWjZS+qv7H3OlUv/6ovUJsAPgZeAzhExI+2aSfZ6UMi+CHLnYJqaYvniU6uJ5+UbtGZmUKcbtBExEBiYtzhpTWA48KuImJ/brR4RIalRh5e7ZW9mBg3ZZ4+kVcgS/V0RcX8Kf5y6YEg/P0nxacBGOad3TbF88a7VxPNysjczg4YcjSPgNuDNiLg2Z9cIoHJETT/gwZz48WlUzq7AvNTdMxI4QFL7dGP2AGBk2jdf0q7pWsfnlFUjd+OYmUFDjrPfnexFNZMkTUix3wJXAsMknUz2voQj075HgYOAyWSv1zwRICLmSLocGJuOuywi5qT1XwB/A9oCj6UlLyd7MzMoqHumEBHxHNQ4fcN+1RwfwBk1lDUIGFRNfBywTV3q5WRvZgYl/wStk72ZGTjZm5mVhVb5b7y2dE72ZmbQYH32zZWTvZkZuBvHzKwsuGVvZlYG3LI3MysDbtmbmZWBWqZBaOmc7M3MwN04ZmZlwd04ZmZlwC17M7My4GRvZlYGfIPWzKwMuM/ezKwMuBvHzKwMlHjLvrS/yszMCiSp4KWAsgZJ+kTSazmxeyVNSMuUylcWStpE0uKcfbfknLODpEmSJku6Pr1zFkkdJI2S9G762b62OjnZm5nRsMme7P2wPXMDEXFURHSPiO7AcOD+nN3vVe6LiNNz4jcDpwLd0lJZ5gXA6IjoBoxO23k52ZuZAapQwUttIuJZYE51+1Lr/EhgaN76SBsAa0XEmPSe2juAw9Lu3sDgtD44J14jJ3szMxq8ZZ/PnsDHEfFuTmxTSa9KekbSnim2ITA155ipKQbQOSJmpPWZQOfaLuobtGZmUKckLqk/0D8nNDAiBhZ4el9WbNXPADaOiNmSdgD+IWnrQusSESEpajvOyd7MjLol+5TYC03uuddoDRwO7JBT1pfAl2n9FUnvAVsA04CuOad3TTGAjyVtEBEzUnfPJ7Vd2904ZmYAqsNSf/sDb0XE8u4ZSetKapXWNyO7Eft+6qaZL2nX1M9/PPBgOm0E0C+t98uJ18jJ3syMBh96ORR4EdhS0lRJJ6ddR/PtG7N7ARPTUMy/A6dHROXN3V8AfwUmA+8Bj6X4lUAPSe+SfYFcWVud3I1jZgZUVDRc2zci+tYQP6Ga2HCyoZjVHT8O2Kaa+Gxgv7rUycnezIy69dm3RE72Zmawsn3xzZ6TvZkZbtmbmZUFJ3szszJQyDQILZmTvZkZbtmbmZUFJ3szszJQ6sm+KE/QShqWs/7HKvueKMY1zcxWRiPOetkkijVdQrec9R5V9q1bpGuamdVf48yN02SK1Y2Tb7rNWqfiNDNrbA05XUJzVKxkv7qkH5L95dA2rVd+J7Yt0jXNzOqtpXbPFKpYyX4mcG0165XbZmbNS2nn+qIl+x4RsaS6HZI2LdI1m7VbfnMIvXb9LrM+W8iOJ98KwLabrcf//boXa7Rtw4cfz+PEK/7BgkVfse8Om3L5qfvQpnUrvlq6jN/+ZTTPvPohAD/stj4Dzz+Utqu2ZuRL73HODdn97vbtVuPO3/XhO+uvw4czP+PYyx7gs8+/aLLPaw1rygfvc945v16+PXXqR/zizP9iwYIFDP/7MDq07wDAWb86mz33+nFTVbNFK/WWfbE6qR6U1KZqUNJ2wFNFumazdufIf9P7gntWiN187sFcdOtT7HTKrYz419v8+qgfATB73iKOuHAYO51yK6de+RCDBvRefs71v+7FGdc8wjbH3czmG3bggJ03B+Dcvrvx9KtT2Pb4m3n61Smc2/dHjffhrOg22XQzht3/IMPuf5Ch993Paqu1Zd/9s7EPxx1/wvJ9TvT159E49TMeeEzS6pUBSXsDjwKnFumazdrzEz9izvzFK8S+27UDz038DwBPvvI+h+25JQD/nvwxM2Z/DsAbU2axWpvWtFmlFet3WJN2q7fh5TenA3D3qIkcuvsWAByy+xYMGTkJgCEjJ3HoHls2yueyxvfSmBfZaKON6NJlw9oPtoI52ddDRFxE1oIfKWlNSYcDdwCHRcSoYlyzJXrzw0+XJ+vDf/x9uq631reO6bPX95jw7ky+WrKMLp3aMW3WguX7ps1aQJdO7QBYr/0azJyTfUHMnPM567VfoxE+gTWFxx97hJ4HHbJ8+5677+KIPody8UUDmD9vXhPWrGVThQpeWqKijTWKiD8ADwCvkL0ya9/01pUaSeovaZykcUunjy1W1ZqN0656mP69d+D5W05izdVX5asly1bY//1NOvGH/vty5nWP1rnsCI9wLUVLvvqKZ556kgMO7AnAkUf15eHHRzFs+IOsu+56XP2nWt9OZzVwy74eJD0kaQSwD9lDVJ8B10oakeLVioiBEbFjROzYustOxahas/LOR7M59Lyh7H76IIY9+TofzPhs+b4NO7Xj3t8fwSn/M4IPpmfx6Z8uYMN1231zzLrtmP5p1tL/ZO5C1u+wJgDrd1iTWZ8tasRPYo3lueee5XtbbU3HTp0A6NipE61ataKiooLDj/gpr02a1MQ1bLka+B20gyR9Ium1nNilkqZJmpCWg3L2DZA0WdLbkg7MifdMscmSLsiJbyrppRS/t7p7pFUVq2V/NXBN+tkH+E3arlwMWHed7JaGBBccuzu3jhgPwNprrMr9/3MUv/vrU7z4+vKX0DNzzucsWPQVO3+/CwA/67EdD7/wDgCPvPAOxx64LQDHHrgtDz//TmN+FGskjz36CL0OOnj59qxZnyxff/Kf/+S73bpVd5oVQCp8KcDfgJ7VxK+LiO5peTS7rrYiexH51umcmyS1ktQKuBHoBWwF9E3HAvwxlfVdYC5wctULVVWUoZcR8Ux1cUkbkX2oaveXssEXHcaeP/gOndZuy+R7z+Lyvz3Lmm3bcFrvHQB48Lm3uePxfwNwep8d2bxLewYctycDjtsTgEPPu5tZny3il39+nIHnH0LbVVfhiZffY+RL7wFw9dAXGXJxH/r16s5/Pp7HsZfd3zQf1Ipm0aJFjHnhBX53yWXLY9dd8yfefustJOjSZUN+d+lleUqwfBqyeyYinpW0SYGH9wbuiYgvgQ8kTQZ2TvsmR8T7qX73AL0lvQnsC/wsHTMYuBS4Od9FVOy+XUnrAj8F+gJdgAci4tzazmu77xXudLZvmfvEhU1dBWuGVmu98o9EbXn+yIJzzjtX9TwN6J8TGhgRA3OPScn+4YjYJm1fCpwAzAfGAedExFxJNwBjImJIOu424LFUTM+IOCXFjwN2IUvsY1KrvrIR/VjldWpSlJa9pHbA4WTfPFsA9wObRkTXYlzPzGxl1aVhnxL7wFoPXNHNwOVk84NdTtalfVIdy6i3Yj1B+wnwMnAR8FxEhKQ+RbqWmdlKqyjykMqI+LhyXdKtwMNpcxqwUc6hXVOMGuKzgXUktY6IpVWOr1GxbtAOAFYFbgIGSNq8SNcxM2sQDXyDtprytUHOZh+gcqTOCOBoSaum6WS6kTWWxwLd0sibNmT3O0dE1vf+FHBEOr8f8GBt1y/WQ1V/johdyW48APwD6CLpfElbFOOaZmYro4GHXg4FXgS2lDRV0snAVZImSZpINiz91wAR8TowDHgDeBw4IyKWpVb7mcBI4E1gWDoW4Hzg7HQztyNwW611KsYNWkkbR8R/qsS2IbtJe1TljYV8fIPWquMbtFadhrhBu+3vRhWccyZd3qPFPVlVrG6cf1SuSBoOEBGvRcSFhSR6M7PGVlFRUfDSEhXrBm3ut95mRbqGmVmDaaGzIBSsMV5L6O4YM2v2WuqcN4UqVrL/gaT5pNcQpnXSdkTEt6d3NDNrQiWe64s2XUKrYpRrZlYsbtmbmZWBEs/1TvZmZlD8J2ibmpO9mRnuxjEzKwslnuud7M3MwC17M7OyUOK53snezAx8g9bMrCy4G8fMrAw42ZuZlYESz/VO9mZm4Ja9mVlZKPFcX7SXl5iZtSgVFSp4qY2kQZI+kfRaTuxPkt6SNFHSA5LWSfFNJC2WNCEtt+Scs0N6leFkSdcr/fkhqYOkUZLeTT/b1/r5Cqj0LyWtpcxtksZLOqDWT2tm1oJUSAUvBfgb0LNKbBSwTURsB7wDDMjZ915EdE/L6Tnxm4FTyV5C3i2nzAuA0RHRDRidtvN/vgIqfVJEzAcOANoDxwFXFnCemVmLIRW+1CYingXmVIk9kV4iDjAG6Jq/PtoAWCsixkT2svA7gMPS7t7A4LQ+OCdeo0KSfeVHOwi4M73dvMR7t8ys3EgqeGkAJwGP5WxvKulVSc9I2jPFNgSm5hwzNcUAOkfEjLQ+E+hc2wULuUH7iqQngE2BAZLaAV8XcJ6ZWYtRlwdoJfUH+ueEBkbEwALPvRBYCtyVQjOAjSNitqQdgH9I2rrQukRESKr19a+FJPuTge7A+xGxSFJH4MRCK2Jm1hLUZbqElNgLSu65JJ0AHALsl7pmiIgvgS/T+iuS3gO2AKaxYldP1xQD+FjSBhExI3X3fFLbtWtM9pK2rxLarNTHoZpZ+VKRe6cl9QTOA34cEYty4usCcyJimaTNyG7Evh8RcyTNl7Qr8BJwPPB/6bQRQD+y+6f9gAdru36+lv01efYFsG9thZuZtRQNOQ+apKHA3kAnSVOBS8hG36wKjEoN5zFp5M1ewGWSlpB1kZ8eEZU3d39BNrKnLVkff2U//5XAMEknAx8CR9ZWpxqTfUTsU8fPZ2bWYjVkz0VE9K0mfFsNxw4HhtewbxywTTXx2cB+dalTIePsV5d0kaSBabubpEPqchEzs+auIYdeNkeFDL28HfgK2C1tTwP+ULQamZk1gQZ+qKrZKSTZbx4RVwFLANKNhZb5ac3MatCQ0yU0R4UMvfxKUluym7JI2pw0TMjMrFS00AZ7wQpJ9pcAjwMbSboL2B04oZiVMjNrbC21e6ZQtSb7iBglaTywK1n3zS8j4tOi18zMrBGVdqovfD77HwN7kHXlrAI8ULQamZk1gVJ/aLTWZC/pJuC7wNAUOk3S/hFxRlFrZmbWiFrofdeCFdKy3xf4fuU8DpIGA68XtVZmZo2spY6yKVQhQy8nAxvnbG+UYmZmJaORpzhudPkmQnuIrI++HfCmpJfT9i7Ay41TPTOzxlHiDfu83ThXN1otzMyaWEttsRcq30RozzRmRczMmlJpp/rCJkLbVdJYSZ9L+krSMknzG6NyZmaNpVWFCl5aokJG49wAHA3cB+xINoH+FsWslJlZYyv1bpxCRuMQEZOBVhGxLCJuB3oWt1pmZo2r1Kc4LqRlv0hSG2CCpKvIXo5b0JeEmVlLUepz4xSStI9Lx50JLCQbZ394MStlZtbYyr5lHxEfptUvgN8DSLoXOKqI9WLuExcWs3hrodrvdGZTV8GaocWv3rDSZTRkn72kQcAhwCcRsU2KdQDuBTYBpgBHRsRcZRf+X+AgYBFwQkSMT+f0Ay5Kxf4hIgan+A58827aR8kmqIx8dapvd8yP6nmemVmz1EoqeCnA3/j2vc0LgNER0Q0YnbYBegHd0tIfuBmWfzlcQvYg687AJZLap3NuBk7NOa/W+6juezczI3uCttClNhHxLDCnSrg3MDitDwYOy4nfEZkxwDqSNgAOBEZFxJyImAuMAnqmfWtFxJjUmr8jp6wa5ZsuYfuadpFNc2xmVjLqMnxeUn+yVnilgRExsJbTOkfEjLQ+E+ic1jcEPso5bmqK5YtPrSaeV74++2vy7HurtoLNzFqSuvTZp8ReW3LPd35IytvH3tDyTZewT2NWxMysKTXCg7EfS9ogImakrphPUnwa2SjHSl1TbBqwd5X40ynetZrj83KfvZkZjTL0cgTQL633Ax7MiR+vzK7AvNTdMxI4QFL7dGP2AGBk2jc/TWUjslkNHqQWhb6W0MyspLVu2KGXQ8la5Z0kTSUbVXMlMEzSycCHwJHp8EfJhl1OJht6eSJARMyRdDkwNh13WURU3vT9Bd8MvXwsLXk52ZuZ0bAPS0VE3xp27VfNsQFU+5rXiBgEDKomPg7Ypi51KmTWS0k6VtLFaXtjSTvX5SJmZs1dhVTw0hIV0md/E9lDVJXfVAuAG4tWIzOzJlD20yUAu0TE9pJeBUiP97Ypcr3MzBpVC52mvmCFJPslklqRvX8WSesCXxe1VmZmjaylvpSkUIUk++uBB4D1JF0BHME3E/OYmZWEEs/1Bc16eZekV8juIgs4LCLeLHrNzMwakUr8LbS1JntJG5ON/XwoNxYR/ylmxczMGlPZt+yBR8j66wWsBmwKvA1sXcR6mZk1qrJP9hGxbe52mg3zF0WrkZlZEyj1F47X+QnaiBgvaZdiVMbMrKm0KvGZwgrpsz87Z7MC2B6YXrQamZk1gZb6ZGyhCmnZt8tZX0rWhz+8ONUxM2saZd1nnx6mahcR5zZSfczMmkSJN+zzvpawdUQslbR7Y1bIzKwpVJTxOPuXyfrnJ0gaAdwHLKzcGRH3F7luZmaNpmxb9jlWA2YD+/LNePsAnOzNrGS0LvFO+3zJfr00Euc1vknylRr1RblmZsVW6i37fCNLWwFrpqVdznrlYmZWMhrq5SWStpQ0IWeZL+lXki6VNC0nflDOOQMkTZb0tqQDc+I9U2yypAtW5vPla9nPiIjLVqZwM7OWoqFa9hHxNtA9K1OtgGlkMwefCFwXEVeveF1tBRxNNgVNF+CfkrZIu28EegBTgbGSRkTEG/WpV75kX+J/1JiZfaNID9DuB7wXER/mmY6hN3BPRHwJfCBpMlD56tfJEfE+gKR70rH1Svb5Pt+3XoxrZlaqivQO2qOBoTnbZ0qaKGmQpPYptiHwUc4xU1Ospni91JjsI2JOfQs1M2tp6pLsJfWXNC5n6V+1vPT61v9HNmwd4GZgc7IunhnANY324ajHRGhmZqWoLu31iBgIDKzlsF7A+Ij4OJ3z8fJrSbcCD6fNacBGOed1TTHyxOusxOd5MzMrjFT4UqC+5HThSNogZ18fsmHtACOAoyWtKmlToBvZQ61jgW6SNk1/JRydjq0Xt+zNzGjY+ewlrUE2iua0nPBVkrqTPac0pXJfRLwuaRjZjdelwBkRsSyVcyYwkmwo/KCIeL2+dXKyNzOjYbs5ImIh0LFK7Lg8x18BXFFN/FHg0Yaok5O9mRmez97MrCz4tYRmZmWg1EerONmbmeGWvZlZWSjtVO9kb2YGQCu37M3MSl+J53onezMzAJV4R46TvZkZpd+yb/TRRpJ+1djXNDOrTQUqeGmJmmJo6dlNcE0zs7yKMBFas9IU3Tgt9J/KzEqZp0toeNEE1zQzy6uitHN9cZK9pAVUn9QFrF6Ma5qZrQyPxqmHiGhXjHLNzIqlxHtxGq8bJ03m3wfoGxEHN9Z1W5qLLxrAs888TYcOHbn/wYeXx+++607uHXoXFRWt2GuvH/Prc89rwlpaQ+naeR3+evnxrNexHREwaPjz3Dj0adqvtTp3/vEkvtOlAx9On8Ox593GZwsWs+cO3bjvuv5MmT4bgAefnMD/DHwcgLce+T0LFn7Jsq+/Zumyr9njmKsAaizLVuSW/UpIr9I6GPgZcCAwHLilmNds6Xofdjh9f3YsFw44f3ns5ZfG8PSTo7nv/hG0adOG2bNnN2ENrSEtXfY1F1x7PxPemsqaq6/KC3efz+iX3uK4Q3fh6Zff5urbR3HuiT0498QDuOj6BwF4/tX3+Mkvq//fqGf//2X2ZwtXiJ17Yo8ay7JvlHqffVGGXko6QNLtwAfAT4A7gDkRcWJEPFSMa5aKHXbcibXWXnuF2H33DuWkU/rTpk0bADp27FjdqdYCzfx0PhPemgrA54u+5K0PZtJl3XU4ZO/tGPLQSwAMeeglDt1nu3pfoyHLKmUVUsFLS1SscfaPA5sBe0TEsSnBf12ka5W8D6dMYfwr4zjm6J9yUr9jeW3SxKaukhXBxht0oPuWXRn72hTW69iOmZ/OB7IvhPU6fnMbbJftNuWley/gHzf8nO9vtv7yeETw0E1n8vxd53HS4bsvj+cry76hOiy1liVNkTRJ0gRJ41Ksg6RRkt5NP9unuCRdL2mypImSts8pp186/l1J/Vbm8xWrG2d7sjeh/1PS+8A9ZC/MzUtSf6A/wA03/YWTT+1fpOq1LEuXLWPevHkMGTqM1yZN4jfn/IpHR44u+fm3y8kabdsw9OpT+M3Vw1mw8Itv7Y80tm3CWx+x5UG/Y+Hirzhwj60Ydl1/tu19GQD7nXgd02fNY932a/LwLWfy9pSZPD/+vRrLshUVocW+T0R8mrN9ATA6Iq6UdEHaPh/oBXRLyy7AzcAukjoAlwA7ko1ufEXSiIiYW5/KFKVlHxETIuKCiNicrLLdgVUkPZYSek3nDYyIHSNiRyf6b3Tu3Jn99u+BJLbdbjsqKiqYO7de/72tGWrduoKhV5/KvY+N48En/w3AJ7MXsH6ntQBYv9NazJqzAIAFC79g4eKvABj53Bus0roVHddZA4Dps+YBMGvu54x4ciI7bb1J3rJsRQ3Zsq9Bb2BwWh8MHJYTvyMyY4B1JG1Adp9zVETMSQl+FNCzvhcv+nQJEfFCRJwFdAWuI/vmsjrYZ7/9Gfty1uc6ZcoHLFmyhPbt2zdxrayh3HLJMbz9wUyuH/Lk8tgjz0zi2EOz/1WOPXQXHn4667rrnNMFs+PW36FCYvZnC1l9tTasufqqAKy+Whv2/9H3eP296XnLsirqkO0l9Zc0Lmep2joN4AlJr+Ts6xwRM9L6TKBzWt8Q+Cjn3KkpVlO8Xor1UNWxETEkre8eEc9HxNdkH36LYlyzVJx/7tmMG/syn302lx777sXPzziLPn1+wsW/+y2H9z6EVVZZhcuvuNJdOCVit+6bccwhuzDpnWmMuecCAC65YQRX3z6KIX88iX6H/Yj/zJjDsecNAqDP/j/k1J/uydJly/jiiyUcP+B2IOuXv/faUwFo3aoV9z42jlEvvDtv/dwAAApWSURBVAlQY1m2orp040TEQGBgnkP2iIhpktYDRkl6q8r5IalRO9QURejAkzQ+Iravul7ddk2+WOppFezb2u90ZlNXwZqhxa/esNKtn7Hvzys45+y02doFX0/SpcDnwKnA3hExI3XTPB0RW0r6S1ofmo5/G9i7comI01J8hePqqljdOKphvbptM7Om10Cd9pLWkNSuch04AHgNGAFUjqjpB1Q+7DACOD6NytkVmJe6e0YCB0hqn0buHJBi9VKs0ThRw3p122ZmTa4Bn6DtDDyQulpbA3dHxOOSxgLDJJ0MfAgcmY5/FDgImAwsAk4EiIg5ki4HxqbjLouIOfWtVLGS/fckTST7Dtw8rZO2NyvSNc3M6q2hboNFxPvAD6qJzwb2qyYewBk1lDUIaJCbLMVK9t8vUrlmZkVR6v3LxZr18sPq4pIqgL5kf8KYmTUbpT7CrVhz46wlaYCkG9I8OZJ0FvA+3/RTmZk1G34tYf3cCcwFXgROAX5L9lfSYRExoUjXNDOrtxaawwtWrGS/WURsCyDpr8AMYOOI+PakH2ZmzUGJZ/tiJfsllSsRsUzSVCd6M2vO/PKS+vmBpPlpXUDbtC2ykUZrFem6Zmb10lL74gtVrNE4tU5nbGbWnDjZm5mVAXfjmJmVAbfszczKQInneid7MzOg5LO9k72ZGUV5B22z4mRvZkbJN+yd7M3MgJLP9k72ZmZ46KWZWVko8S77or2D1sysRWmgV9AiaSNJT0l6Q9Lrkn6Z4pdKmiZpQloOyjlngKTJkt6WdGBOvGeKTZZ0wcp8Prfszcxo0JeXLAXOiYjx6cXjr0galfZdFxFXV7nuVsDRwNZAF+CfkrZIu28EegBTgbGSRkTEG/WplJO9mRkN+g7aGWTTuhMRCyS9CWyY55TewD0R8SXwgaTJwM5p3+T0Tlsk3ZOOrVeydzeOmRl168aR1F/SuJylf7VlSpsAPwReSqEzJU2UNEhS+xTbEPgo57SpKVZTvF6c7M3MoE7ZPiIGRsSOOcvAbxUnrQkMB34VEfOBm4HNge5kLf9rGuFTLeduHDMzGnbopaRVyBL9XRFxP0BEfJyz/1bg4bQ5Ddgo5/SuKUaeeJ25ZW9mRsO9cFzZnd7bgDcj4tqc+AY5h/UBXkvrI4CjJa0qaVOgG/AyMBboJmlTSW3IbuKOqO/nc8vezAyoaLiG/e7AccAkSRNS7LdAX0ndgQCmAKcBRMTrkoaR3XhdCpwREcsAJJ0JjARaAYMi4vX6VkoRUd9zi+qLpTTPilmTar/TmU1dBWuGFr96w0qn6qlzvyo453Rt36bFPYLllr2ZGaX/BK2TvZkZJT8PmpO9mRm4ZW9mVhYacLqEZsnJ3swMd+OYmZWFEm/YO9mbmYFfXmJmVh5KO9c72ZuZQcnneid7MzOAihLvtHeyNzOj9G/QetZLM7My4Ja9mRml37J3sjczw0MvzczKglv2ZmZlwMnezKwMuBvHzKwMuGVvZlYGSjzXO9mbmQEln+2d7M3MKP3pEhRR8AvVrYlI6h8RA5u6Hta8+PfC6sLTJbQM/Zu6AtYs+ffCCuZkb2ZWBpzszczKgJN9y+B+WauOfy+sYL5Ba2ZWBtyyNzMrA072ZmZlwMm+iUhaJmlCzrJJiv9K0heS1s45dm9JD+ds/0HS45JWlfS0pLdzyvl7438aawg5vxOvSXpI0jopvomkxVV+X47POa+7pJDUs0p5nzf2Z7Dmy0/QNp3FEdG9mnhfYCxwOHB71Z2SLgJ2Bw6KiC+VPfV3TESMK2ZlrVEs/52QNBg4A7gi7Xuvht8XyH5nnks/Hy96La1Fcsu+GZG0ObAmcBHZ/7hV958D9AIOjYjFjVw9a1wvAhvWdpCyb/ufAicAPSStVuR6WQvlZN902ub8Sf5Aih0N3AP8C9hSUuec43cHTgd6RUTVP8/vyinrT8WvuhWTpFbAfsCInPDmVbpx9kzx3YAPIuI94Gng4MatrbUU7sZpOtV14/QF+kTE15KGk7XYbkj7JgPtgR7A8CrnuRunNLSVNIGsRf8mMCpnX03dOH3JGgikn8fz7d8PMyf75kLStkA3YFTqh28DfMA3yf5j4BhgtKQ5EfFUk1TUimlxRHSXtDowkqzP/vqaDk5/AfwE6C3pQrJJejtKahcRCxqlxtZiuBun+egLXBoRm6SlC9BF0ncqD4iId8hu3A6RVNPNOmvhImIR8F/AOZLyNcj2AyZGxEbpd+Y7ZK36Po1RT2tZnOybj6OBB6rEHkjx5SJiLHAiMCLd0IUV++z/WfyqWrFFxKvARL65UV+1z/6/0r6qvzPDc85ZXdLUnOXsxqm9NUeeLsHMrAy4ZW9mVgac7M3MyoCTvZlZGXCyNzMrA072ZmZlwMneVlBl5sX70gM+9S3rb5KOSOt/lbRVnmP3lrRbPa4xRVKnQuM1lHGCpBtqP7J+5Zs1B072VtXiiOgeEdsAX5HNx7NcLQ/51CgiTomIN/IcsjfZPC9mVgRO9pbPv4Dvplb3vySNAN6Q1ErSnySNlTRR0mmQzcAo6YY0v/4/gfUqC0rz7u+Y1ntKGi/p35JGp7n8Twd+XTnJl6R1JQ1P1xgrafd0bkdJT0h6XdJfyaYIKIiknSW9KOlVSS9I2jJn90apju9KuiTnnGMlvZzq9Zc0RUFumWtIeiR9ltckHVXHf2OzRuG5caxaqQXfi2/mR98e2CYiPpDUH5gXETtJWhV4XtITwA+BLYGtgM7AG8CgKuWuC9wK7JXK6hARcyTdAnweEVen4+4GrouI5yRtTDZXzPeBS4DnIuIySQcDJ9fhY70F7BkRSyXtD/w32dwyADsD2wCLgLGSHgEWAkcBu0fEEkk3kc1PdEdOmT2B6RFxcKr32pg1Q072VlXlzIuQtexvI+teeTkiPkjxA4DtKvvjgbXJJnHbCxgaEcuA6ZKerKb8XYFnK8uKiDk11GN/YKs0KRzAWpLWTNc4PJ37iKS5dfhsawODJXUDAlglZ9+oiJgNIOl+YA9gKbADWfIHaAt8UqXMScA1kv4IPBwR/6pDfcwajZO9VfWtqZdToluYGwLOioiRVY47qAHrUQHsGhFfVFOX+roceCoi+qSuo6dz9lWdNyTIPufgiBhQU4ER8Y6k7YGDgD9IGh0Rl61MJc2KwX32Vh8jgZ9LWgVA0haS1gCeBY5KffobAPtUc+4YYC9Jm6ZzO6T4AqBdznFPAGdVbuTM8vks8LMU60U2x3+h1gampfUTquzrIamDpLbAYcDzwGjgCEnrVdZVObOQplgXYFFEDAH+RNbdZdbsuGVv9fFXYBNgvLKm9iyyBPkAsC9ZX/1/yF6tt4KImJX6/O+XVEHWLdIDeAj4u6TeZEn+v4AbJU0k+z19luwm7u+BoZJeB15I16nJRElfp/VhwFVk3TgXAY9UOfZlshkjuwJDKl8Gk459ItV1Cdkc8x/mnLct8Kd0nSXAz/PUx6zJeNZLM7My4G4cM7My4GRvZlYGnOzNzMqAk72ZWRlwsjczKwNO9mZmZcDJ3sysDPx/yvjTpXjuLrQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8pf0z78jJFl"
      },
      "source": [
        "Finally for evaluation, the best model is picked previously saved and evaluate it against the test dataset. I use a default threshold of 0.5 to decide when to classify a sample as FAKE. If the model output is greater than 0.5, news is classified as FAKE; otherwise, REAL. The output of the classification report indicating the precision, recall, and F1-score for each class, as well as the overall accuracy. The one layer biLSTM model gives the accuracy of 99% when tested on test dataset."
      ]
    }
  ]
}